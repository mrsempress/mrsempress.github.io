<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">


  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="paper,Object detection," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="This is my blog. 沉迷于为什么我的3D框这么混乱（就有很多个很多个），特别是有多余的框，但是学长的就那么正常，那么干净。干净了许多，虽然还有一些奇怪的框，不过毕竟切入角度不一样，释然了">
<meta name="keywords" content="paper,Object detection">
<meta property="og:type" content="article">
<meta property="og:title" content="3DObjectDetection">
<meta property="og:url" content="http://mrsempress.top/2020/08/29/3DObjectDetection/index.html">
<meta property="og:site_name" content="Mrs_empress">
<meta property="og:description" content="This is my blog. 沉迷于为什么我的3D框这么混乱（就有很多个很多个），特别是有多余的框，但是学长的就那么正常，那么干净。干净了许多，虽然还有一些奇怪的框，不过毕竟切入角度不一样，释然了">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://mrsempress.top/2020/08/29/3DObjectDetection/Qin2020_1.png">
<meta property="og:image" content="http://mrsempress.top/2020/08/29/3DObjectDetection/Qin2020_4.png">
<meta property="og:image" content="http://mrsempress.top/2020/08/29/3DObjectDetection/Qin2020_2.png">
<meta property="og:image" content="http://mrsempress.top/2020/08/29/3DObjectDetection/Qin2020_3.png">
<meta property="og:image" content="http://mrsempress.top/2020/08/29/3DObjectDetection/Arsalan_2017_1.png">
<meta property="og:image" content="http://mrsempress.top/2020/08/29/3DObjectDetection/Arsalan_2017_2.png">
<meta property="og:image" content="http://mrsempress.top/2020/08/29/3DObjectDetection/Arsalan_2017_3.png">
<meta property="og:image" content="http://mrsempress.top/2020/08/29/3DObjectDetection/Chen_2016_1.png">
<meta property="og:image" content="http://mrsempress.top/2020/08/29/3DObjectDetection/Chen_2016_2.png">
<meta property="og:image" content="http://mrsempress.top/2020/08/29/3DObjectDetection/li_2020_1.png">
<meta property="og:image" content="http://mrsempress.top/2020/08/29/3DObjectDetection/li_2020_2.png">
<meta property="og:image" content="http://mrsempress.top/2020/08/29/3DObjectDetection/li_2020_3.png">
<meta property="og:image" content="http://mrsempress.top/2020/08/29/3DObjectDetection/li_2020_4.png">
<meta property="og:image" content="http://mrsempress.top/2020/08/29/3DObjectDetection/li_2020_5.png">
<meta property="og:image" content="http://mrsempress.top/2020/08/29/3DObjectDetection/li_2020_6.png">
<meta property="og:image" content="http://mrsempress.top/2020/08/29/3DObjectDetection/li_2020_7.png">
<meta property="og:image" content="http://mrsempress.top/2020/08/29/3DObjectDetection/li_2020_8.png">
<meta property="og:image" content="http://mrsempress.top/2020/08/29/3DObjectDetection/li_2020_9.png">
<meta property="og:image" content="http://mrsempress.top/2020/08/29/3DObjectDetection/IDA_2020_1.png">
<meta property="og:image" content="http://mrsempress.top/2020/08/29/3DObjectDetection/IDA_2020_2.png">
<meta property="og:image" content="http://mrsempress.top/2020/08/29/3DObjectDetection/Liu2020_1.png">
<meta property="og:image" content="http://mrsempress.top/2020/08/29/3DObjectDetection/GN.png">
<meta property="og:image" content="http://mrsempress.top/2020/08/29/3DObjectDetection/Gaussian%20Kernel.png">
<meta property="og:image" content="http://mrsempress.top/2020/08/29/3DObjectDetection/SMOKE_2020_1.png">
<meta property="og:image" content="http://mrsempress.top/2020/08/29/3DObjectDetection/OFT_2018_1.png">
<meta property="og:image" content="http://mrsempress.top/2020/08/29/3DObjectDetection/OFT_2018_2.png">
<meta property="og:image" content="http://mrsempress.top/2020/08/29/3DObjectDetection/Brazil_2019_1.png">
<meta property="og:image" content="http://mrsempress.top/2020/08/29/3DObjectDetection/Brazil_2019_2.png">
<meta property="og:image" content="http://mrsempress.top/2020/08/29/3DObjectDetection/Brazil_2019_3.png">
<meta property="og:image" content="http://mrsempress.top/2020/08/29/3DObjectDetection/Brazil_2019_4.png">
<meta property="og:image" content="http://mrsempress.top/2020/08/29/3DObjectDetection/Tracking_2019_1.png">
<meta property="og:image" content="http://mrsempress.top/2020/08/29/3DObjectDetection/ColorEmbedded_1.png">
<meta property="og:image" content="http://mrsempress.top/2020/08/29/3DObjectDetection/ColorEmbedded_2.png">
<meta property="og:image" content="http://mrsempress.top/2020/08/29/3DObjectDetection/MonoDIS_2019_1.png">
<meta property="og:image" content="http://mrsempress.top/2020/08/29/3DObjectDetection/MonoDIS_2019_2.png">
<meta property="og:image" content="http://mrsempress.top/2020/08/29/3DObjectDetection/MonoDIS_2019_4.jpg">
<meta property="og:image" content="http://mrsempress.top/2020/08/29/3DObjectDetection/MonoDIS_2019_3.png">
<meta property="og:image" content="http://mrsempress.top/2020/08/29/3DObjectDetection/GS3D_2019_1.png">
<meta property="og:image" content="http://mrsempress.top/2020/08/29/3DObjectDetection/GS3D_2019_3.png">
<meta property="og:image" content="http://mrsempress.top/2020/08/29/3DObjectDetection/GS3D_2019_2.png">
<meta property="og:updated_time" content="2020-12-06T09:44:57.590Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="3DObjectDetection">
<meta name="twitter:description" content="This is my blog. 沉迷于为什么我的3D框这么混乱（就有很多个很多个），特别是有多余的框，但是学长的就那么正常，那么干净。干净了许多，虽然还有一些奇怪的框，不过毕竟切入角度不一样，释然了">
<meta name="twitter:image" content="http://mrsempress.top/2020/08/29/3DObjectDetection/Qin2020_1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.2',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://mrsempress.top/2020/08/29/3DObjectDetection/"/>





  <title>3DObjectDetection | Mrs_empress</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0b0957531a34243a173c768258ed03c4";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->




</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <a href="https://mrsempress.github.io/" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#fff; color:#151513; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Mrs_empress</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Your bright sun</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-poem">
          <a href="/poem" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-book"></i> <br />
            
            poem
          </a>
        </li>
      
        
        <li class="menu-item menu-item-resume">
          <a href="http://mrsempress-certificate.oss-cn-beijing.aliyuncs.com/%E9%BB%84%E6%99%A8%E6%99%B0.pdf" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            resume
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mrsempress.top/2020/08/29/3DObjectDetection/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mrs_empress">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mrs_empress">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">3DObjectDetection</h1>
        

        <div class="post-meta">
	  
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-08-29T16:12:37+08:00">
                2020-08-29
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/计算机视觉/" itemprop="url" rel="index">
                    <span itemprop="name">计算机视觉</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/计算机视觉/Object-detection/" itemprop="url" rel="index">
                    <span itemprop="name">Object detection</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/2020/08/29/3DObjectDetection/" class="leancloud_visitors" data-flag-title="3DObjectDetection">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>This is <a href="https://mrsempress.github.io" target="_blank" rel="external">my blog</a>.</p>
<p><del>沉迷于为什么我的3D框这么混乱（就有很多个很多个），特别是有多余的框，但是学长的就那么正常，那么干净。</del>干净了许多，虽然还有一些奇怪的框，不过毕竟切入角度不一样，释然了</p>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<a id="more"></a>
<h1 id="paper"><a href="#paper" class="headerlink" title="paper"></a>paper</h1><h2 id="MonoGRNet-A-Geometric-Reasoning-Network-for-Monocular-3D-Object-Localization——2019"><a href="#MonoGRNet-A-Geometric-Reasoning-Network-for-Monocular-3D-Object-Localization——2019" class="headerlink" title="*MonoGRNet: A Geometric Reasoning Network for Monocular 3D Object Localization——2019"></a>*MonoGRNet: A Geometric Reasoning Network for Monocular 3D Object Localization——2019</h2><p>AAAI 2019 oral</p>
<p><strong>意义：</strong>为什么要研究3D目标检测，为什么要研究深度呢？因为在图像中，传统的物体定位或检测估计二维边界框，可以框住属于图像平面上物体的可见部分。但是，这种检测结果<strong><em>无法在真实的 3D 世界中提供场景理解的几何感知</em></strong>，这对很多应用的意义并不大。换句话来说，我知道在某个方位有个目标存在，但是我不知道它离我的距离，不知道他的实际大小，甚至这个方位是不准确的。因此3D目标检测是必要的，带深度的3D目标检测对于自动驾驶来说，意义也是重大的。</p>
<p><strong>概括：</strong>这篇文章通过将网络<strong>解耦</strong>为<strong>四个渐进式子任务</strong>，分别是<strong>2D目标检测、实例级深度估计IDE、3D位置检测、局部角落回归</strong>。在<strong><em>检测</em></strong> 到的 2D 边界框的引导下，网络首先<strong><em>估计</em></strong> 3D 框中心的深度和 2D 投影以<strong><em>获得全局 3D 位置</em></strong>，然后在本地环境中<strong><em>回归</em></strong>各个角坐标。最终的 3D 边界框基于估计的 3D 位置和局部角落在全局环境中以<strong><em>端到端</em></strong>的方式进行<strong><em>优化</em></strong>。</p>
<p><img src="/2020/08/29/3DObjectDetection/Qin2020_1.png" alt=""></p>
<p><strong>几个重点：</strong></p>
<ul>
<li><p>将 <strong>3D 定位问题解耦</strong>为几个<strong>渐进式</strong>子任务，分别为2D目标检测<script type="math/tex">B_{2d}</script>、实例级深度估计IDE<script type="math/tex">Z_c</script>、3D位置检测（2D中心点<script type="math/tex">c</script>包含<script type="math/tex">u,v</script>两个方向，3D中心点<script type="math/tex">C</script>包含<script type="math/tex">X,Y,Z</script>三个方向）、局部角落回归<script type="math/tex">\mathcal{O}</script>（公8个点）。每个子任务从单目 RGB 图像中学习，通过<strong>几何推断</strong>，在<strong><em>已观察到的二维投影平面</em></strong>和在<strong><em>未观察到的深度维度</em></strong>中定位物体<strong>非模态三维边界框（Amodal Bounding Box, ABBox-3D）</strong>，即实现了由二维视频确定物体的三维位置。</p>
<script type="math/tex; mode=display">
B_{3d} = (B_{2d}, Z_c, c, \mathcal{O})</script></li>
<li><p>提出了<strong>instance depth estimation(IDE)</strong>，使用3D bounding box的中心点通过<strong><em>（稀疏地）监督</em></strong>来预测目标的深度，不需要考虑目标的规模以及2D的位置；之间的都是pixel-to-pixel，而在一张图片中，背景占大部分，因此我们用像素级别，并且通过平均误差来计算损失，实际上是不准确的；IDE模块探索深度特征映射的大型感知域以<strong><em>捕获粗略的实例深度</em></strong>，然后联合<strong><em>更高分辨率的早期特征</em></strong> 以<strong><em>优化 IDE</em></strong>。<del>(可以看到深层网络提取的是粗略的全局信息，然后通过浅层的有细节的高分辨率的局部信息来修正，使得信息更加精确)，有点像centernet，通过中心回归。</del></p>
<p><img src="/2020/08/29/3DObjectDetection/Qin2020_4.png" alt=""></p>
<script type="math/tex; mode=display">
Z_c = Z_{cc}+\delta_{Z_c}</script></li>
<li><p><strong>全局3D位置</strong>，用<strong><em>公式将2D和3D的中心点进行转换</em></strong>（2D投影的中心点和3D的中心点不同），为了<strong>同时检索水平和垂直位置</strong>，首先要<strong>预测 3D 中心的 2D 投影</strong>。结合 IDE，然后将<strong><em>投影中心拉伸到真实 3D空间</em></strong> 以获得最终的3D对象位置。</p>
<p><img src="/2020/08/29/3DObjectDetection/Qin2020_2.png" style="zoom:35%;"><img src="/2020/08/29/3DObjectDetection/Qin2020_3.png" style="zoom:35%;"></p>
<script type="math/tex; mode=display">
u=f_x*X/Z+p_x,\ v=f_y*Y/Z+p_y</script><p>其中，<script type="math/tex">f_x, f_y</script>分别表示在X和Y轴上的焦距，<script type="math/tex">p_x,p_y</script>是坐标系的原点（coordinates of the principle point）</p>
<p>这样在2D投影到3D的时候，就可以用：</p>
<script type="math/tex; mode=display">
X=(u-p_x)*Z/f_x,\ Y=(v-p_y)*Z/f_y</script><p>和IDE模块相似，我们使用early features来回归得到<script type="math/tex">\delta_C</script>，这样中心点的3D位置就可以用<script type="math/tex">C=C_s+\delta_C</script>。</p>
</li>
<li><p><strong>局部角落回归</strong>，用高分辨率的信息来回归局部3D框的角点，根据图二的C图，从局部坐标到相机坐标的转换涉及到旋转<script type="math/tex">R</script>和平移<script type="math/tex">C</script>，全局角落坐标为<script type="math/tex">O_k^{cam}=RO_k+C</script>。</p>
</li>
<li><p><strong>统一的网络结构，端到端训练</strong>，通过<strong>联合的几何损失函数</strong>进行优化，最大限度地减少 3D 边界在整体背景下的边界框的差异。</p>
<ul>
<li><p>2D检测（softmax[<script type="math/tex">s\cdot</script>] cross entropy[CE] loss + masked L1 distance[d] loss）：</p>
<script type="math/tex; mode=display">
\mathcal L_{conf}=CE_{g\in\mathcal G}(s\cdot(Pr^g_{obj}),\tilde{Pr}^g_{obj})\\
\mathcal L_{bbox}=\sum_g\mathbb {1}_g^{obj}\cdot d(B_{2d}^g,\tilde B_{2d}^g)\\
\mathcal L_{2d} = \mathcal L_{conf} + \omega\mathcal L_{bbox}</script><p>其中，Pr是置信度，<script type="math/tex">\mathbb 1_g^{obj}</script>是指示格子g中是否属于任何目标，如果格子g到最近的目标b的距离小于<script type="math/tex">\sigma_{scope}</script>，那么设置为1。</p>
</li>
<li><p>实例深度预测（L1 loss）：</p>
<script type="math/tex; mode=display">
\mathcal L_{zc}=\sum_g\mathbb {1}_g^{obj}\cdot d(Z_{cc}^g,\tilde Z_{c}^g)\\
\mathcal L_{z\delta}=\sum_g\mathbb {1}_g^{obj}\cdot d(Z_{cc}^g+\delta^g_{Z_c},\tilde Z_{c}^g)\\
\mathcal L_{depth} = \alpha\mathcal L_{zc} + \mathcal L_{z\delta}</script><p>其中，<script type="math/tex">\alpha>1</script>，鼓励模型在粗略估计的时候已经接近真实值了。</p>
</li>
<li><p>3D 定位损失（L1 loss）：</p>
<script type="math/tex; mode=display">
\mathcal L_c^{2d}=\sum_g\mathbb {1}_g^{obj}\cdot d(g+\delta^g_{c},\tilde c{^g})\\
\mathcal L_{c}^{3d}=\sum_g\mathbb {1}_g^{obj}\cdot d(C_{s}^g+\delta^g_{C},\tilde C^g)\\
\mathcal L_{location} = \beta\mathcal L_{c}^{2d} + \mathcal L_{c}^{3d}</script><p>其中，<script type="math/tex">\beta>1</script>，鼓励模型在学习投影中心点时已经接近真实值了。</p>
</li>
<li><p>局部角点损失（L1 loss）：</p>
<script type="math/tex; mode=display">
\mathcal L_{corners}=\sum_g\sum_k\mathbb 1^{obj}_g\cdot d(O_k,\tilde O_k)</script></li>
<li><p>联合3D损失：</p>
<script type="math/tex; mode=display">
\mathcal L_{joint}=\sum_g\sum_k\mathbb 1^{obj}_g\cdot d(O_k^{cam},\tilde O_k^{cam})</script></li>
</ul>
</li>
</ul>
<p><strong>效果：</strong>在KITTI 数据集上，该网络在 3D 物体定位方面优于<strong>最先进</strong>的单目方法，且<strong>推理时间最短</strong>。</p>
<h2 id="Deep-3D-box-3d-Bounding-Box-Estimation-Using-Deep-Learning-and-Geometry——2017"><a href="#Deep-3D-box-3d-Bounding-Box-Estimation-Using-Deep-Learning-and-Geometry——2017" class="headerlink" title="*Deep 3D box: 3d Bounding Box Estimation Using Deep Learning and Geometry——2017"></a>*Deep 3D box: 3d Bounding Box Estimation Using Deep Learning and Geometry——2017</h2><p>这篇文章的特点在于提出的几何限制关系，以及提出了预测dimension这一stable属性。</p>
<p>In contrast to <strong>current techniques</strong> that <strong>only regress the 3D orientation</strong> of an object, <strong>our method</strong> ﬁrst regresses <strong>relatively stable 3D object properties</strong> using a deep convolutional neural network and then combines these estimates <strong>with geometric constraints</strong> provided by a <strong>2D object bounding box</strong> to produce a complete 3D bounding box. 之前的方法主要在意object的方向，但这篇文章增添了一些object的固定属性：choose to regress the <strong>box dimensions D</strong> rather than <strong><em>translation T</em></strong> because the variance of the dimension estimate is typically smaller (e.g. cars tend to be <strong><em>roughly the same size</em></strong>)。</p>
<p>同时利用了2D bbox和3D bbox之间的几何限制关系来限制9个DOF(three for translation, three for rotation, and three for box dimensions)。</p>
<p>总体结构使用了MultiBin的思想，proposed <strong>MultiBin</strong> architecture for orientation estimation. We ﬁrst <strong>discretize the orientation angle</strong> and <strong>divide it into n overlapping bins</strong>. For each bin, the CNN network estimates both <strong>a conﬁdence probability</strong> <script type="math/tex">c_i</script> that the output angle lies inside the <script type="math/tex">i^{th}</script> bin and the <strong>residual rotation</strong> correction that needs to be applied to the orientation of the center ray of that bin in order to obtain the output angle. The residual rotation is represented by two numbers, for the <strong>sine and the cosine</strong> of the angle. V<strong>alid cosine and sine</strong> values are obtained by applying an <strong>L2 normalization layer</strong> on top of a 2-dimensional input. This results in <strong>3 outputs for each bin</strong> <script type="math/tex">i:(c_i, \cos(\Delta \theta_i), sin(\Delta \theta_i))</script>. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_bins</span><span class="params">(bins)</span>:</span></div><div class="line">    angle_bins = np.zeros(bins)</div><div class="line">    interval = <span class="number">2</span> * np.pi / bins</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,bins):</div><div class="line">        angle_bins[i] = i * interval</div><div class="line">    angle_bins += interval / <span class="number">2</span> <span class="comment"># center of the bin</span></div><div class="line"></div><div class="line">    <span class="keyword">return</span> angle_bins</div></pre></td></tr></table></figure>
<ul>
<li>The ﬁrst network output estimates the <strong>3D object orientation</strong> using a novel <strong>hybrid discrete-continuous loss</strong>, which signiﬁcantly outperforms the <strong>L2 loss</strong>. </li>
<li>The second output <strong>regresses the 3D object dimensions</strong>, which have <strong><em>relatively little variance</em></strong> compared to alternatives and can often be predicted for many object types. These estimates, <strong>combined with the geometric constraints</strong> (use the fact that the <strong><em>perspective projection of a 3D bounding box</em></strong> should <strong>ﬁt tightly</strong> within its <strong><em>2D detection window</em></strong>) on translation imposed by the 2D bounding box, enable us to recover a stable and accurate 3D object pose. </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span><span class="params">(nn.Module)</span>:</span></div><div class="line">      <span class="comment"># ...</span></div><div class="line">      <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        x = self.features(x)  <span class="comment"># 512 x 7 x 7</span></div><div class="line">        x = x.view(<span class="number">-1</span>, <span class="number">512</span> * <span class="number">7</span> * <span class="number">7</span>)</div><div class="line">        orientation = self.orientation(x)</div><div class="line">        orientation = orientation.view(<span class="number">-1</span>, self.bins, <span class="number">2</span>)  <span class="comment"># angle ssin + cos</span></div><div class="line">        orientation = F.normalize(orientation, dim=<span class="number">2</span>)</div><div class="line">        confidence = self.confidence(x)</div><div class="line">        dimension = self.dimension(x)</div><div class="line">        <span class="keyword">return</span> orientation, confidence, dimension</div></pre></td></tr></table></figure>
<p><img src="/2020/08/29/3DObjectDetection/Arsalan_2017_1.png" style="zoom:50%;"><img src="/2020/08/29/3DObjectDetection/Arsalan_2017_2.png" style="zoom:50%;"></p>
<p>Loss for the MultiBin <strong>orientation</strong> is thus:</p>
<script type="math/tex; mode=display">
L_\theta = L_{conf} + w \times L_{loc}\\
L_{loc} = -\frac 1 {n_{\theta^*}}\sum cos(\theta^* − c_i − \Delta\theta_i )</script><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">OrientationLoss</span><span class="params">(orient, angleDiff, confGT)</span>:</span></div><div class="line">    <span class="comment">#</span></div><div class="line">    <span class="comment"># orient = [sin(delta), cos(delta)] shape = [batch, bins, 2]</span></div><div class="line">    <span class="comment"># angleDiff = GT - center, shape = [batch, bins]</span></div><div class="line">    <span class="comment">#</span></div><div class="line">    [batch, _, bins] = orient.size()</div><div class="line">    cos_diff = torch.cos(angleDiff)</div><div class="line">    sin_diff = torch.sin(angleDiff)</div><div class="line">    cos_ori = orient[:, :, <span class="number">0</span>]</div><div class="line">    sin_ori = orient[:, :, <span class="number">1</span>]</div><div class="line">    mask1 = (confGT != <span class="number">0</span>)</div><div class="line">    mask2 = (confGT == <span class="number">0</span>)</div><div class="line">    count = torch.sum(mask1, dim=<span class="number">1</span>)</div><div class="line">    tmp = cos_diff * cos_ori + sin_diff * sin_ori</div><div class="line">    tmp[mask2] = <span class="number">0</span></div><div class="line">    total = torch.sum(tmp, dim=<span class="number">1</span>)</div><div class="line">    count = count.type(torch.FloatTensor).cuda()</div><div class="line">    total = total / count</div><div class="line">    <span class="keyword">return</span> -torch.sum(total) / batch</div><div class="line">  </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">OrientationLoss</span><span class="params">(orient_batch, orientGT_batch, confGT_batch)</span>:</span></div><div class="line">    batch_size = orient_batch.size()[<span class="number">0</span>]</div><div class="line">    indexes = torch.max(confGT_batch, dim=<span class="number">1</span>)[<span class="number">1</span>]</div><div class="line"></div><div class="line">    <span class="comment"># extract just the important bin</span></div><div class="line">    orientGT_batch = orientGT_batch[torch.arange(batch_size), indexes]</div><div class="line">    orient_batch = orient_batch[torch.arange(batch_size), indexes]</div><div class="line"></div><div class="line">    theta_diff = torch.atan2(orientGT_batch[:, <span class="number">1</span>], orientGT_batch[:, <span class="number">0</span>])</div><div class="line">    estimated_theta_diff = torch.atan2(orient_batch[:, <span class="number">1</span>], orient_batch[:, <span class="number">0</span>])</div><div class="line"></div><div class="line">    <span class="keyword">return</span> <span class="number">-1</span> * torch.cos(theta_diff - estimated_theta_diff).mean()</div><div class="line">  </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">orientation_loss</span><span class="params">(y_true, y_pred)</span>:</span></div><div class="line">    <span class="comment"># Find number of anchors</span></div><div class="line">    anchors = tf.reduce_sum(tf.square(y_true), axis=<span class="number">2</span>)</div><div class="line">    anchors = tf.greater(anchors, tf.constant(<span class="number">0.5</span>))</div><div class="line">    anchors = tf.reduce_sum(tf.cast(anchors, tf.float32), <span class="number">1</span>)</div><div class="line"></div><div class="line">    <span class="comment"># Define the loss</span></div><div class="line">    loss = (y_true[:,:,<span class="number">0</span>]*y_pred[:,:,<span class="number">0</span>] + y_true[:,:,<span class="number">1</span>]*y_pred[:,:,<span class="number">1</span>])</div><div class="line">    loss = tf.reduce_sum((<span class="number">2</span> - <span class="number">2</span> * tf.reduce_mean(loss,axis=<span class="number">0</span>))) / anchors</div><div class="line"></div><div class="line">    <span class="keyword">return</span> tf.reduce_mean(loss)</div></pre></td></tr></table></figure>
<p>其中<script type="math/tex">L_{conf}</script> is equal to the <strong>softmax loss</strong> of the conﬁdences of each bin <code>nn.CrossEntropyLoss()</code>. <script type="math/tex">L_{loc}</script> is the loss that tries to <strong>minimize</strong> the difference between the estimated <strong>angle</strong> and the ground truth angle in each of the bins that covers the ground truth angle, with adjacent bins having overlapping coverage.</p>
<p>由于下述关系，<script type="math/tex">L_{loc}</script>就只关联为cos部分。<del>首先(gt_theta-pred_theta) 是在一个bins中与中心角的差值，所以在bins=2时，它的范围在[-pi/2, pi/2]，那么希望loss接近-1，即这一项趋向于0，即括号中的项应该是相同的；这里的theta是一个结果，output，而不是中间的值，所以根据导数反向传播修改的是内部参数，而因为内部参数变了，导致这次预测的theta是更靠近结果的</del></p>
<p><img src="/2020/08/29/3DObjectDetection/Arsalan_2017_3.png" style="zoom:50%;"></p>
<p>The loss for <strong>dimension</strong> estimation <script type="math/tex">L_{dims}</script> is computed as follows(L2 loss <code>nn.MSELoss()</code>):</p>
<script type="math/tex; mode=display">
L_{dims}=\sum (D^∗− \bar D − \delta)^2 ,</script><p><strong>几何限制关系</strong>：例如：<script type="math/tex">x_0=[d_x/2, -d_y/2, d_z/2]^T</script>的投影应该和2D框的最左边tightly。【同样对<script type="math/tex">x_{max},y_{min},y_{max}</script>也有类似的限制】</p>
<script type="math/tex; mode=display">
x_{min} = (k\left[\begin{matrix}R&T\end{matrix}\right]\left[\begin{matrix}d_x/2\\-d_y/2\\d_z/2\\1\end{matrix}\right])_x</script><p><strong>Total loss</strong> is the weighted combination of </p>
<script type="math/tex; mode=display">
L = \alpha × L_{dims} + L_{\theta}</script><hr>
<p>Introduce three additional <strong>performance metrics</strong> measuring the 3D box accuracy: <strong>distance</strong> to <strong><em>center</em></strong> of box, <strong>distance</strong> to the center of the <strong><em>closest</em></strong> bounding box face, and the over-all bounding box <strong>overlap</strong> with the ground truth box, measured using 3D Intersection over Union (3D IoU) score.</p>
<h2 id="Mono3d：Monocular-3D-Object-Detection-for-Autonomous-Driving——2016"><a href="#Mono3d：Monocular-3D-Object-Detection-for-Autonomous-Driving——2016" class="headerlink" title="Mono3d：Monocular 3D Object Detection for Autonomous Driving——2016"></a>Mono3d：Monocular 3D Object Detection for Autonomous Driving——2016</h2><p>主要思路就是利用<strong>3D包围框与2D包围框之间存在的映射联系</strong>，用2D空间中的特征来描述3D包围框。First aims to generate a set of <strong>candidate class-speciﬁc object proposals</strong>提出一种生成类相关的物体推荐候选框算法, which are then run through <em>a standard CNN</em> pipeline to obtain <strong>high-quality object detections.</strong>然后利用标准CNN来获得高质量的目标检测。The focus of this paper is on <strong><em>proposal generation</em></strong>. Propose <strong>an energy minimization approach</strong> that places object candidates in 3D using the fact that objects should be on the ground-plane利用<strong><em>物体和地平面接触</em></strong>的约束条件. We then <strong>score each candidate box</strong> projected to the image plane <strong>via several intuitive potentials</strong> encoding semantic segmentation, contextual information, size and location priors and typical object shape. 用SSVM对物体框打分，选取得分高的框进行分类和精细化的调整。</p>
<p>两个假设：</p>
<p>（三维空间<strong>离散化</strong>为立体像素边长为0.2m的三维模型）</p>
<ol>
<li>由于KITTI数据集的单目图像拍摄时相机在汽车上<strong>固定位置</strong>，因此地平面相对于相机的映射关系是固定的，同时作者假设所有图像的相机Y轴（也就是三维空间中的垂直方向）与地面是<strong>垂直</strong>的。</li>
<li>所有的物体<strong>底部</strong>都<strong>与地平面相接</strong>。</li>
</ol>
<p><img src="/2020/08/29/3DObjectDetection/Chen_2016_1.png" style="zoom:70%;"></p>
<ul>
<li><p><strong>sample candidate bounding boxes</strong> with typical physical sizes in the 3D space by assuming a prior on the ground-plane. Represent each object with <strong>a 3D bounding box, y = (x, y, z, θ, c, t)</strong>(θ: azimuth angle, c: class, t: representative 3D templates)采样的时候认为不同类别的物体拥有不同的高度范围，这个范围误差在高斯函数的范围内。密集采样之后将内部像素完全是地面的物体框去除，同时去除具有非常低的3D位置先验概率的框。由于密集的采样（穷举），候选框使用<strong>积分图</strong>进行特征提取。</p>
</li>
<li><p><strong>project the boxes</strong> to the image plane, thus avoiding multi-scale search in the image</p>
</li>
<li><p><strong>score candidate boxes</strong> by exploiting multiple features: <strong><em>class semantic, instance semantic, contour, object shape, context, and location prior</em></strong>. </p>
<p>scoring function:</p>
<script type="math/tex; mode=display">
E(x, y) =w^T_{c,sem}\phi_{c,sem}(x, y) + w^T_{c,inst}\phi_{c,inst}(x, y)+ w^T_{c,cont}\phi_{c,cont}(x, y) + w^T_{c,loc}\phi_{c,loc}(x, y)+w^T_{c,shape}\phi_{c,shape}(x, y)</script><ul>
<li><p>class semantic</p>
<p>语义分割主要有两个衡量，一个是框内属于c的像素比例，另一个是框内不属于c的像素比例。</p>
<p>Incorporate two types of features encoding semantic segmentation. The ﬁrst feature encourages the presence of an object inside the bounding box by <strong>counting the percentage of pixels</strong> labeled as the <strong>relevant class</strong>:</p>
<script type="math/tex; mode=display">
\phi_{c,seg}(x, y)=\frac{\sum_{i\in\Omega(y)}S_c(i)}{|\Omega(y)|}</script><p>The second feature computes the <strong>fraction of pixels</strong> that belong to <strong>classes</strong> other than the object class:</p>
<script type="math/tex; mode=display">
\phi_{c,non−seg,c'}(x, y)=\frac{\sum_{i\in\Omega(y)}S_c'(i)}{|\Omega(y)|}</script></li>
<li><p>instance semantic</p>
<p>只对汽车进行实例分割</p>
</li>
<li><p>shape</p>
<p>在2D的候选物体包围框中划分了两种栅格，其中，一种栅格图含有一个栅格，另一种栅格图含有K×K个栅格，统计这些<strong>栅格中每个栅格内的轮廓像素数量</strong>。</p>
</li>
<li><p>context</p>
<p>用2D物体包围框的下方的1/3高度的区域作为上下文区域，用汽车下必然是地面这一上下文关系作为约束</p>
</li>
<li><p>location</p>
<p>使用<strong>核密度估计(KDE)</strong>学习物体的位置先验信息，其中固定位置偏差为4m，2D图像位置偏差为两个像素</p>
</li>
</ul>
<p><strong>Weight</strong> each loss <strong>equally</strong>, and deﬁne the <strong>category loss</strong> as <strong>cross entropy</strong>, the <strong>orientation loss</strong> as a smooth l1 and the <strong>bounding box offset loss</strong> as a <strong>smooth l1</strong> loss over the 4 coordinates that parameterized the 2D bounding box.</p>
</li>
<li><p>利用CNN对高分框<strong>重新打分和分类</strong>，得到候选框的类别、精确位置以及方向信息</p>
<p><img src="/2020/08/29/3DObjectDetection/Chen_2016_2.png" style="zoom:50%;"></p>
</li>
<li><p>A ﬁnal set of object proposals is obtained after <strong>non-maximum suppression</strong></p>
</li>
</ul>
<h2 id="RTM3D-Real-time-Monocular-3D-Detection-from-Object-Keypoints-for-Autonomous-Driving——2020"><a href="#RTM3D-Real-time-Monocular-3D-Detection-from-Object-Keypoints-for-Autonomous-Driving——2020" class="headerlink" title="RTM3D: Real-time Monocular 3D Detection from Object Keypoints for Autonomous Driving——2020"></a>RTM3D: Real-time Monocular 3D Detection from Object Keypoints for Autonomous Driving——2020</h2><p>这一篇文章主要是将3D Detection问题转换为<strong>预测关键点问题</strong>；首先通过预测object的9个关键点（8个角点+1个中心点），然后通过<strong>几何关系</strong>（9个关键点，18个限制）得到dimension, location, and orientation。</p>
<p>整个网络由3个部分组成：backbone, keypoint feature pyramid, and detection head. 总体是one-stage的。</p>
<p><img src="/2020/08/29/3DObjectDetection/li_2020_1.png" style="zoom:50%;"></p>
<ul>
<li><p><strong><em>backbone</em></strong>: ResNet-18 and DLA-34</p>
</li>
<li><p><strong><em>keypoint feature pyramid</em></strong>: Keypoint in the image have <strong>no difference in size</strong>. Therefore, the keypoint detection is not suitable for using the Feature Pyramid Network(FPN). Instead, propose <strong>Keypoint Feature Pyramid Network (KFPN)</strong> to detect scale-invariant key-points in the <em>point-wise space</em>. </p>
<ul>
<li>F scale feature maps, ﬁrst resize <strong>each scale</strong> f, <strong>back</strong> to the size of <strong>maximal scale</strong>, then yields the <strong>feature maps</strong> <script type="math/tex">f_{1<f<F}</script> </li>
<li>generate soft weight by a <strong>softmax</strong> operation to denote the <strong>importance of each scale</strong></li>
<li><strong>scale-space score map S</strong> score is obtained by <strong>linear weighing sum</strong></li>
</ul>
<script type="math/tex; mode=display">
S_{score} = \sum_f \hat f\odot softmax(\hat f), \odot\text{:element-wise product}</script></li>
<li><p><strong><em>detection head</em></strong></p>
<ul>
<li><strong>three</strong> fundamental components<ul>
<li>Inspired by CenterNet, we take <strong><em>a keypoint</em></strong> as the <strong>main-center</strong> for connecting all features. The heatmap can be deﬁne as <script type="math/tex">M \in [0, 1]^{\frac H S \times \frac W S \times C}</script> , where C is the number of object categories</li>
<li><strong>heatmap of nine perspective points</strong> <script type="math/tex">V \in [0, 1]^{\frac H S \times \frac W S \times 9 }</script> projected by <strong>vertexes</strong> and <strong>center</strong> of 3D bounding box</li>
<li>For keypoints association of one object,  <strong>regress an local offset</strong> <script type="math/tex">V_c \in R^{\frac H S \times \frac W S \times 18}</script> from the maincenter as an indication</li>
</ul>
</li>
<li><strong>six</strong> optional components<ul>
<li>The <strong>center offset</strong> <script type="math/tex">M_{os} \in R^{\frac H S \times \frac W S \times 2}</script> and <strong>vertexes offset</strong> <script type="math/tex">V_{os} \in R^{\frac H S \times \frac W S \times 2}</script>  are discretization error for each keypoint in heatmaps<del>(这个和local offser有什么区别，一个是离main center点的offset，这个是从heatmap来看？)</del></li>
<li>The <strong>dimension</strong> <script type="math/tex">D \in R^{\frac H S \times \frac W S \times 3}</script> of 3D object have <strong>a smaller variance</strong>, which makes it easy to predict.</li>
<li>The <strong>rotation</strong> <script type="math/tex">R(\theta)</script> of an object only by parametrized by <strong>orientation</strong> <script type="math/tex">\theta (yaw)</script>. <strong>Multi-Bin</strong> based method to regress the <strong>local orientation.</strong> generates feature map of <script type="math/tex">O \in R ^{\frac H S \times \frac W S \times 8}</script> orientation with two bins.<del>2个bins4个值；加上confidence也是4个，总共8个</del>。</li>
<li>regress <script type="math/tex">Z\in R^{\frac H S \times\frac W S \times1}</script> the <strong>depth</strong> of 3D box center.</li>
</ul>
</li>
</ul>
<p>获取bbox的：</p>
<ul>
<li>main-center, center offset, wh</li>
</ul>
<p>获取3d corners：</p>
<ul>
<li>Vertexes,  vertexes offset, vertexes coordinate</li>
</ul>
<p>画图的时候未用到<del>所以这些是在哪里起到限制作用的呢，（这些只是为了得到3D的信息）？</del>：</p>
<ul>
<li>orientation, dimension, depth</li>
</ul>
</li>
</ul>
<p>Loss:</p>
<ul>
<li><p>The all <strong>heatmaps of keypoint</strong> training strategy(focal loss)</p>
<p>The loss solves the <strong>imbalance</strong> of positive and negative samples with <strong>focal loss</strong>.</p>
<p><img src="/2020/08/29/3DObjectDetection/li_2020_2.png" style="zoom:43%;"></p>
<p>K is the <strong>channels</strong> of different keypoints, K = C in maincenter and K = 9 in vertexes. N is the <strong>number</strong> of maincenter or vertexes in an image, and <script type="math/tex">\alpha</script> and <script type="math/tex">\beta</script> are the hyper-parameters to reduce the loss weight of negative and easy positive samples. We set is <script type="math/tex">\alpha = 2</script> and <script type="math/tex">\beta = 4</script>. <script type="math/tex">p_{kxy}</script> can be deﬁned by <strong>Gaussian kernel</strong> <script type="math/tex">p_{xy} = exp(-\frac{x^2+y^2}{2\sigma})</script> centered by ground truth keypoint <script type="math/tex">\tilde p_{xy}</script>. For <script type="math/tex">\sigma</script>, we ﬁnd the <strong>max area</strong> <script type="math/tex">A_{max}</script> and <strong>min area</strong> <script type="math/tex">A_{min}</script> of 2D box in training data and set two hyperparameters <script type="math/tex">\sigma_{max}</script> and <script type="math/tex">\sigma_{min}</script>. We then deﬁne the <script type="math/tex">\sigma = A( \frac{\sigma_{max} - \sigma_{min}}{A_{max} - A_{min}})</script> for a object with size A.</p>
</li>
<li><p><strong>Regression</strong> of <strong>dimension and distance</strong>(residual term)</p>
<p><img src="/2020/08/29/3DObjectDetection/li_2020_3.png" style="zoom:43%;"></p>
<script type="math/tex; mode=display">
\Delta \tilde D_{xy}=\log \frac{\tilde D_{xy}-\bar D}{D_{\sigma}}</script><script type="math/tex; mode=display">
1^{obj}_{xy}\text{ if maincenter appears in position x, y.}</script></li>
<li><p>offset of maincenter, vertexes(L1)</p>
<p><img src="/2020/08/29/3DObjectDetection/li_2020_4.png" style="zoom:43%;"></p>
</li>
<li><p>coordinate of vertexes(L1)</p>
<p><img src="/2020/08/29/3DObjectDetection/li_2020_5.png" style="zoom:43%;"></p>
</li>
<li><p>All(multi-task)</p>
<p><img src="/2020/08/29/3DObjectDetection/li_2020_6.png" style="zoom:43%;"></p>
</li>
</ul>
<p>Our goal is to estimate the <strong>3D bounding box</strong>, whose projections of center and 3D vertexes on the image space best <strong>ﬁt</strong> the corresponding <strong>2D keypoint</strong>. We formulate it and other prior errors as a <strong>nonlinear least squares optimization</strong> problem:</p>
<p>由特征点检测网络给出9个特征 $\widehat{k p}_{i j}$ 点、物体尺寸 $D_{i} 、$ 方向 $\hat{\theta}_{i}$ 和中心点深度 $\widehat{Z}_{i}$ 后, $3 \mathrm{D}$ BBox就容易求了，如下式所示。 $R^{<em>}, T^{</em>}, D^{*}=\underset{\{R, T, D\}}{\arg \min } \sum_{R_{i}, T_{i}, D_{i}}\left|e_{c p}\left(R_{i}, T_{i}, D_{i}, \widehat{k p}_{i}\right)\right|_{\Sigma_{i}}^{2}+\omega_{d}\left|e_{d}\left(D_{i}, \widehat{D}_{i}\right)\right|_{2}^{2}+\omega_{r}\left|e_{r}\left(R_{i}, \hat{\theta}_{i}\right)\right|_{2}^{2}$<br>其中 $e_{c p}$ 是3D BBox的八个顶点和中心点到 $\widehat{k p}_{i j}$ 的重投影误差。八个顶点和中心点可以通过 $R, T, D$ 得到。 $\quad \Sigma_{i}=\operatorname{diag}\left(\operatorname{softmax}\left(V\left(\widehat{k p}_{i}\right)\right)\right.$ 为特征点检测的置信度，这里可以衡量各误差</p>
<p><img src="/2020/08/29/3DObjectDetection/li_2020_7.png" style="zoom:43%;"></p>
<p>the <strong>covariance matrix</strong> of keypoints <strong>projection error</strong>.</p>
<p><img src="/2020/08/29/3DObjectDetection/li_2020_8.png" style="zoom:43%;"></p>
<p><img src="/2020/08/29/3DObjectDetection/li_2020_9.png" style="zoom:43%;"></p>
<p>一些error的计算，SE3 space: special euclidian 3-space.</p>
<p>Evaluation:</p>
<ul>
<li>Average precision for 3D <strong>intersection-over-union</strong> (AP 3D)</li>
<li>Average precision for <strong>Birds Eye View</strong> (AP BEV )</li>
<li>Average <strong>Orientation Similarity</strong> (AOS) if 2D bounding box available.</li>
</ul>
<p><del>非官方复现上，对于depth使用了sigmoid，应该是为了平滑吧</del></p>
<p><del>一个问题就是得到这么多信息后，但是复原image并没有都用上，那为什么需要预测呢？感觉复原image的那部分预测的是在camera坐标系下的，后面是实际3D，但这样就不算3D了</del></p>
<h2 id="IDA-3D-Instance-Depth-Aware-3D-Object-Detection-from-Stereo-Vision-for-Autonomous-Driving"><a href="#IDA-3D-Instance-Depth-Aware-3D-Object-Detection-from-Stereo-Vision-for-Autonomous-Driving" class="headerlink" title="IDA-3D: Instance-Depth-Aware 3D Object Detection from Stereo Vision for Autonomous Driving"></a>IDA-3D: Instance-Depth-Aware 3D Object Detection from Stereo Vision for Autonomous Driving</h2><p><strong>stereo</strong> images, <strong>not rely on depth</strong>,  A stereo RPN module is introduced to produce <strong>a pair of union RoI</strong> to <strong><em>avoid complex matching</em></strong> of the same object in a left-right image pair(关注instance not pixel) and <strong><em>reduce the interference of background</em></strong> on depth estimation. Pays more attention on <strong>far-away objects</strong> by <strong><em>disparity adaptation</em></strong> and <strong><em>matching cost reweighting.</em></strong> 这篇文章主要就是用instance简化了pixel，然后用了概率去选择一个depth level（其实也是个multibin的思想），同时根据disparity和depth的关系，修正了转换关系，最后加上了左右图之间关系连接的一个weight</p>
<p><img src="/2020/08/29/3DObjectDetection/IDA_2020_1.png" style="zoom:50%;"></p>
<ul>
<li><p>design a <strong>six-parallel fully-connected network</strong>?(5个+1个RPN？)</p>
<ul>
<li>角度multibins</li>
<li>维度，预测<script type="math/tex">\Delta</script>，然后用平均值与e的<script type="math/tex">\Delta</script>次方的乘积作为实际值</li>
<li>Multi-task loss</li>
</ul>
</li>
<li><p>computing the <strong>disparity</strong> of <strong>each instance</strong> to locate its position.<code>disparity×height×width×feature size</code></p>
<ul>
<li><p>employ <strong><em>two consecutive 3D convolution layers</em></strong>, each followed by <strong><em>a 3D max-pooling layer</em></strong></p>
</li>
<li><p>监督学习预object中心点的depth；N个depth level（实验中分了24个）, 给出每一个<strong>可能性</strong>，最后预测的depth</p>
<script type="math/tex; mode=display">
\hat z=\sum_{i=0}^N z_iP(i)</script><p>然后和gt进行smooth L1损失计算</p>
</li>
</ul>
</li>
<li><p>修正depth：for <strong>the same disparity error</strong>, the error in <strong>depth increases quadratically with distance</strong>. It means that the inﬂuence of the disparity error in depth estimation of a far-away object is greater than a nearby one.如下图所示</p>
<p><img src="/2020/08/29/3DObjectDetection/IDA_2020_2.png" style="zoom:50%;"></p>
<ul>
<li><p>In order to adapt the model and loss function to <strong>lay more emphasis on a far-away object</strong>, we change the disparity level in cost volume from uniform quantization to <strong>nonuniform quantization</strong> where the farther the object is, <strong>the less the partition</strong> cell between two <strong>consecutive</strong> disparity levels.转换公式：</p>
<script type="math/tex; mode=display">
D=\frac {f_u\times b}{z}</script><p><script type="math/tex">f_u</script>: horizontal focal length, b: baseline of binocular camera</p>
<p>我们不必估算0-80m范围内的深度，因为汽车的深度与图像中的尺寸成反比?Given camera intrinsic parameters, we can roughly calculate the range according to the <strong>width</strong> of the union box in the image.只评估在一定范围内的</p>
</li>
</ul>
</li>
<li><p>Cost weighting</p>
<p>To <strong>penalize the depth levels</strong> that are <strong>not unique</strong> for an object instance and <strong>promote</strong> the depth levels that have <strong>high prob-abilities</strong>, we reweight the matching cost.</p>
<p>The ﬁrst part in 4D volume <strong>packing a difference feature map</strong> between the left and right feature maps across each disparity level（in a certain <strong>depth level</strong> and <strong>reﬁne</strong> depth estimation） and second part in 3DCNN <strong>employing attention mechanism</strong> on depth（sets the weight <script type="math/tex">r_i</script> for each channel）. The correlation score <script type="math/tex">r_i</script>  that is obtained by <strong>calculating the correlation</strong> between left and right feature maps on each disparity is deﬁned as: 就是左右两边特征图求cos相似性</p>
<script type="math/tex; mode=display">
r_i = cos<F^l_i,F^r_i>=\frac{F^l_i\cdot F^r_i}{\|F^l_i\|\cdot \|F^r_i\|}</script></li>
</ul>
<h2 id="SMOKE-Single-Stage-Monocular-3D-Object-Detection-via-Keypoint-Estimation——2020"><a href="#SMOKE-Single-Stage-Monocular-3D-Object-Detection-via-Keypoint-Estimation——2020" class="headerlink" title="SMOKE: Single-Stage Monocular 3D Object Detection via Keypoint Estimation——2020"></a>SMOKE: Single-Stage Monocular 3D Object Detection via Keypoint Estimation——2020</h2><p>old: In case of <strong>monocular vision</strong>, successful methods have been mainly based on <strong>two ingredients</strong>: (i) a network <strong>generating 2D region proposals</strong>, (ii) a R-CNN structure <strong>predicting 3D object pose</strong> by utilizing the acquired regions of interest.</p>
<p>This: predicts a 3D bounding box for each detected object by <strong>combining a single keypoint estimate with regressed 3D variables</strong>. As a second contribution, we propose a <strong>multi-step disentangling approach</strong> for constructing the 3D bounding box, which signiﬁcantly <strong><em>improves</em></strong> both <strong><em>training convergence</em></strong> and <strong><em>detection accuracy</em></strong>. In contrast to previous 3D detection techniques, our method does <strong><em>not require</em></strong> complicated <strong><em>pre/post-processing, extra data, and a reﬁnement stage</em></strong>. 总结来说就是包含关键点预测和3D变量回归两个模块的一阶段单目3D检测方法。</p>
<p><img src="/2020/08/29/3DObjectDetection/Liu2020_1.png" style="zoom:57%;"></p>
<hr>
<ul>
<li><p>Problem: A single RGB image <script type="math/tex">I \in R^{W\times H\times3}</script> , ﬁnd for each present object its category label C and its 3D bounding box B, where the latter is parameterized by 7 variables <script type="math/tex">(h, w, l, x, y, z, θ)</script>. (<script type="math/tex">(x, y, z)</script> is the coordinates (in meters) of the object center in the <strong>camera coordinate</strong> frame.)</p>
</li>
<li><p>Backbone: <strong><em>DLA-34</em></strong> since it can <strong>aggregate</strong> information across <strong>different layers</strong>.</p>
<ul>
<li><p>all the <strong><em>hierarchical aggregation connections</em></strong> are replaced by a <strong>Deformable Convolution Network (DCN</strong>). </p>
</li>
<li><p>The output feature map is <strong>downsampled 4 times</strong> with respect to the original image.</p>
</li>
<li><p>Replace all <strong><em>BatchNorm (BN)</em></strong> operation with <strong>GroupNorm (GN)</strong>, since less sensitive to <em>batch size</em> and more robust to <em>training noise</em></p>
<ul>
<li><p>BN以batch的维度做归一化，依赖batch，<strong>过小</strong>的batch size会导致其<strong>性能下降</strong>，如果<strong>太大</strong>，<strong>显存</strong>又可能不够用，一般来说每GPU上batch设为32最合适。但这个维度并不是固定不变的，比如训练和测试时一般不一样，一般都是训练的时候在训练集上通过滑动平均预先计算好平均-mean，和方差-variance参数。而在测试的时候，不再计算这些值，而是直接调用这些预计算好的来用，但当训练数据和测试<strong>数据分布有差别时</strong>，训练时上预计算好的数据并不能代表测试数据，这就导致在训练，验证，测试这三个阶段存在不一致。</p>
</li>
<li><p>GN：同样可以解决<strong>Internal Covariate Shift</strong>的问题，channel方向每个group的均值和方差</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">GroupNorm</span><span class="params">(x, gamma, beta, G, eps=<span class="number">1e-5</span>)</span>:</span></div><div class="line">    <span class="comment"># x: input features with shape [N,C,H,W]</span></div><div class="line">    <span class="comment"># gamma, beta: scale and offset, with shape [1,C,1,1]</span></div><div class="line">    <span class="comment"># G: number of groups for GN</span></div><div class="line">    N, C, H, W = x.shape</div><div class="line">    x = tf.reshape(x, [N, G, C // G, H, W])</div><div class="line">    mean, var = tf.nn.moments(x, [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], keep dims=<span class="keyword">True</span>)</div><div class="line">    x = (x - mean) / tf.sqrt(var + eps)</div><div class="line">    x = tf.reshape(x, [N, C, H, W])</div><div class="line">    <span class="keyword">return</span> x * gamma + beta</div></pre></td></tr></table></figure>
</li>
</ul>
<blockquote>
<p>BatchNorm：batch方向做归一化，算<script type="math/tex">N*H*W</script>的均值<br>LayerNorm：channel方向做归一化，算<script type="math/tex">C*H*W</script>的均值<br>InstanceNorm：一个channel内做归一化，算<script type="math/tex">H*W</script>的均值<br>GroupNorm：将channel方向分group，然后每个group内做归一化，算<script type="math/tex">(C//G)*H*W</script>的均值</p>
<p>GN比LN效果好的原因是GN比LN的限制更少，因为LN假设了一个层的所有通道的数据共享一个均值和方差。而IN则丢失了探索通道之间依赖性的能力。</p>
</blockquote>
<p><img src="/2020/08/29/3DObjectDetection/GN.png" style="zoom:50%;"></p>
</li>
</ul>
</li>
<li><p>3D Detection Network</p>
<ul>
<li><p><strong>Keypoint Branch</strong>: the key point is deﬁned as the <strong>projected 3D center</strong> of the object on the image plane.</p>
<script type="math/tex; mode=display">\left[\begin{matrix}x&y&z\end{matrix}\right]^T$$ represent the 3D center of each object in the camera frame. The projection of 3D points to points  $$\left[\begin{matrix}x_c&y_c\end{matrix}\right]^T$$on the image plane can be obtained with the camera intrinsic matrix K in a homogeneous form:</script><p>\left[\begin{matrix}z\cdot x\\z\cdot y\\z\end{matrix}\right]=K_{3\times 3}\left[\begin{matrix}x\\y\\z\end{matrix}\right]<br>$$<br><strong>Downsampled location</strong> on the feature map is computed and distributed using a <strong>Gaussian Kernel</strong>.</p>
<p><img src="/2020/08/29/3DObjectDetection/Gaussian Kernel.png" style="zoom:50%;"></p>
</li>
<li><p><strong>Regression Branch</strong>: the 3D information is encoded  as an 8-tuple <script type="math/tex">\tau =\left[\begin{matrix}\delta_z&\delta_{x_c}&\delta_{y_c}&\delta_h&\delta_w&\delta_l&\sin\alpha&\cos \alpha\end{matrix}\right]^T</script> . <script type="math/tex">\delta_{x_c} , \delta_{y_c}</script> is the <strong>discretization offset</strong> due to <strong><em>downsampling</em></strong>, <script type="math/tex">\delta_h,\delta_w,\delta_l</script> denotes the <strong>residual dimensions</strong>. We encode all variables to be learnt in <strong>residual representation</strong> to reduce the learning interval and ease the training task. A similar operation F that converts <strong>projected 3D points to a 3D bounding box</strong> <script type="math/tex">B = F(\tau) \in \mathbb R^{3\times 8}</script> . </p>
<ul>
<li><p>For each object, its <strong>depth z</strong> can be recovered by predeﬁned <strong>scale and shift</strong> parameters <script type="math/tex">\sigma_z, \mu_z</script> as</p>
<script type="math/tex; mode=display">
z=\mu_z+\delta_z\sigma_z</script><p>这样上述的the location for each object in the camera frame可以表示为：</p>
<script type="math/tex; mode=display">
\left[\begin{matrix}x\\y\\z\end{matrix}\right]=K_{3\times 3}^{-1}\left[\begin{matrix}z\cdot (x_c+\delta_{x_c})\\z\cdot (y_c+\delta_{y_c})\\z\end{matrix}\right]</script><p>对于dimensions的部分，使用每一个类别预先设定好的平均值（从整个数据集中先计算平均值）<strong>pre-calculated category-wise average dimension</strong>作为一个base，然后用residual representation来计算出真实的维度。</p>
<script type="math/tex; mode=display">
\left[\begin{matrix}h\\w\\l\end{matrix}\right]=\left[\begin{matrix}\bar h\cdot e^{\delta_h}\\\bar w\cdot e^{\delta_w}\\\bar l\cdot e^{\delta_l}\end{matrix}\right]</script><p>对于角度部分，Regress the <strong>observation angle</strong> <script type="math/tex">\alpha</script> instead of the yaw rotation <script type="math/tex">\theta</script> for each object. [from. “3D bounding box estimation using deep learning and geometry”]. We further change the observation angle <strong>with respect to the object head</strong> <script type="math/tex">\alpha_x</script> , instead of the commonly used observation angle value <script type="math/tex">\alpha_z</script> , by simply <strong>adding <script type="math/tex">\frac \pi 2</script> .</strong></p>
<p><img src="/2020/08/29/3DObjectDetection/SMOKE_2020_1.png" style="zoom:35%;"></p>
<p>因此最终预测的yaw：</p>
<script type="math/tex; mode=display">
\theta = \alpha_z + arctan(\frac{x}{z})</script><p>Bounding box：</p>
<script type="math/tex; mode=display">
B=R_{\theta}\left[\begin{matrix}\pm h/2\\\pm w/2\\\pm l/2\end{matrix}\right]+\left[\begin{matrix}x\\y\\z\end{matrix}\right]</script></li>
</ul>
</li>
</ul>
</li>
<li><p>Loss</p>
<ul>
<li><p>Keypoint Classiﬁcation Loss</p>
<p><strong>Penalty-reduced focal loss</strong> in a <strong>point-wise</strong>. <script type="math/tex">s_{i,j}</script> be the <strong><em>predicted</em></strong> score at the heatmap location <script type="math/tex">(i, j)</script> and <script type="math/tex">y_{i,j}</script> be the <strong><em>ground-truth</em></strong> value of each point assigned by <strong>Gaussian Kernel</strong>.</p>
<p>定义<script type="math/tex">\breve y_{i,j},\breve s_{i,j}</script>：</p>
<script type="math/tex; mode=display">
\breve y_{i,j}=\begin{cases}0,\ if\ y_{i,j}=1\\y_{i,j},\ otherwise \end{cases}\\
\breve s_{i,j}=\begin{cases}s_{i,j},\ if\ y_{i,j}=1\\1-s_{i,j},\ otherwise \end{cases}</script><p>简化为同一个类别，则分类损失函数：</p>
<script type="math/tex; mode=display">
L_{cls}=-\frac 1 N\sum_{i,j=1}^{h,w}(1-\breve y_{i,j})^{\beta}(1-\breve s_{i,j})^{\alpha}\log(\breve s_{i,j})</script><p>N是图片中object的数量，即<script type="math/tex">y_{i,j}=1</script>的个数；The term <script type="math/tex">(1 − y_{i,j} )</script> corresponds to penalty reduction for points around the groundtruth location. <del>从整个损失函数来看，中心点希望score尽可能的大，远离部分的点，希望score尽可能的小</del></p>
</li>
<li><p>Regression Loss（observe that l1 loss performs better than Smooth l1 loss）</p>
<p>add <strong>channel-wise activation</strong> to the regressed parameters of dimension and orientation <strong><em>at each feature map location</em></strong> to <strong>preserve consistency</strong>. The activation functions for the <strong><em>dimension</em></strong> and the <strong><em>orientation</em></strong> are chosen to be the <strong><em>sigmoid function <script type="math/tex">\sigma</script> and the <script type="math/tex">l2</script> norm</em></strong>, respectively:</p>
<script type="math/tex; mode=display">
\left[\begin{matrix}\delta_h\\\delta_w\\\delta_l\end{matrix}\right]=\sigma\left(\begin{matrix}o_h\\o_w\\o_l\end{matrix}\right)-\frac 1 2\\
\left[\begin{matrix}\sin \alpha\\\cos \alpha\end{matrix}\right]=\left[\begin{matrix}o_{\sin}/\sqrt{o_{sin}^2+o^2_{cos}}\\o_{\cos} /\sqrt{o_{sin}^2+o^2_{cos}}\end{matrix}\right]</script><p>o stands for the speciﬁc <strong>output of network</strong>. The 3D bounding box regression loss as the <strong>l1 distance</strong> between pred and gt. λ is a scaling factor.</p>
<script type="math/tex; mode=display">
L_{reg}=\frac \lambda N\|\hat B-B\|_1</script></li>
<li><p><strong>The ﬁnal loss function</strong>(three different groups: orientation, dimension and location.)</p>
<script type="math/tex; mode=display">
L = L_{cls} + \sum_{i=1}^3 L_{reg}(\hat B_i)</script></li>
</ul>
</li>
</ul>
<hr>
<p>细节处理：</p>
<ul>
<li><p>数据处理：去除3D中心点在图像外侧的框</p>
</li>
<li><p>数据增强：random horizontal ﬂip, random scale(9 steps from 0.6 to 1.4) and shift(5 steps from -0.2 to 0.2). Note that the <strong>scale and shift</strong> augmentation methods are only used for <strong>heatmap classiﬁcation</strong> since the 3D information becomes <strong>inconsistent</strong> with data augmentation. <del>应该是指数据增强部分只是为了hm分类，并不作用到3D检测上；从代码来看，增加了一个flag，相当于最后预测完之后，再逆方向返回，得到未增强后的3D信息</del></p>
</li>
<li><p>超参数：In the backbone, the group number for <strong>GroupNorm is set to 32</strong>. For channels less than 32, it is set to be 16. Set <script type="math/tex">\alpha = 2, \beta = 4, \left[\begin{matrix}\bar h& \bar w&\bar l\end{matrix}\right]^T =\left[\begin{matrix}1.63&1.53&3.88\end{matrix}\right]^T, \left[\begin{matrix}\mu_z&\sigma_z\end{matrix}\right]^T=\left[\begin{matrix}28.01&16.32\end{matrix}\right]^T</script>. </p>
</li>
<li><p>训练：use the original image resolution and pad it to <strong>1280 × 384</strong>. The <strong>learning rate</strong> is set at <script type="math/tex">2.5 × 10^{−4}</script> and <strong>drops</strong> at 25 and 40 epochs by a factor of 10. </p>
</li>
<li><p>测试：Use the top 100 detected 3D projected points and ﬁlter it with <strong>a threshold of</strong></p>
<p><strong>0.25</strong>. <strong>No data augmentation</strong> method and NMS are used in the test procedure.</p>
</li>
</ul>
<h2 id="Shift-R-CNN"><a href="#Shift-R-CNN" class="headerlink" title="Shift R-CNN"></a>Shift R-CNN</h2><p>第1阶段：faster R-CNN，增加3D角度和尺寸回归。 第2阶段：用相机投影几何约束进行3D平移的闭式解方案。第3阶段：ShiftNet细化和最终3D目标框重建。</p>
<h2 id="GS3D"><a href="#GS3D" class="headerlink" title="GS3D"></a>GS3D</h2><p>大范围的回归通常不比离散分类好，因此将残差回归转换为3D边框细化的分类过程。其主要思想是将残差范围划分为多个区间，识别残差值是位于其中一个区间。</p>
<h2 id="OFT-Orthographic-Feature-Transform-for-Monocular-3D-Object-Detection——2018"><a href="#OFT-Orthographic-Feature-Transform-for-Monocular-3D-Object-Detection——2018" class="headerlink" title="OFT: Orthographic Feature Transform for Monocular 3D Object Detection——2018"></a>OFT: Orthographic Feature Transform for Monocular 3D Object Detection——2018</h2><p>One explanation for this performance gap(lidar-based with image-based) is that existing systems are entirely at the mercy of the <strong>perspective image-based representation</strong>, in which the appearance and scale of objects varies <strong>drastically with depth</strong> and meaningful distances are <strong>difﬁcult to infer.</strong></p>
<p>Escape the image domain by mapping image-based features <strong>into an orthographic 3D space.</strong></p>
<hr>
<p><img src="/2020/08/29/3DObjectDetection/OFT_2018_1.png" alt=""></p>
<p>five main components:</p>
<ul>
<li><p>Front-end ResNet without bottleneck layers: feature extractor, a hierarchy of multi-scale, aim to eliminate variance to scale</p>
</li>
<li><p>Orthographic feature transform: a differentiable transformation which maps a set of features extracted from a perspective RGB image <strong>to an orthographic birds-eye-view feature map.</strong></p>
<p><img src="/2020/08/29/3DObjectDetection/OFT_2018_2.png" alt=""></p>
<ul>
<li><p><strong>3D voxel feature map <script type="math/tex">g(x,y,z)\in \mathbb R^n</script></strong> with relevant <strong>n-dimensional features</strong> from the image-based feature map <script type="math/tex">f(u,v)\in \mathbb R^n</script>. The voxel map is defined over a <strong>uniformly</strong> spaced 3D lattice <script type="math/tex">\mathcal G</script>.  ﬁxed to the ground plane a distance y0 <strong>below</strong> the camera and has dimensions W, H, D and a voxel size of r.</p>
<ul>
<li><p><strong>approximate  r</strong> by a rectangular bounding box</p>
<script type="math/tex; mode=display">
\begin{aligned}
&u_{1}=f \frac{x-0.5 r}{z+0.5 \frac{x}{|x|} r}+c_{u}, \quad v_{1}=f \frac{y-0.5 r}{z+0.5 \frac{y}{|y|} r}+c_{v}\\
&u_{2}=f \frac{x+0.5 r}{z-0.5 \frac{x}{|x|} r}+c_{u}, \quad v_{2}=f \frac{y+0.5 r}{z-0.5 \frac{y}{|y|} r}+c_{v}
\end{aligned}</script><p>f: camera focal length c: principle point</p>
</li>
<li><p><strong>appropriate location</strong> in the voxel feature map g by average pooling over the projected voxel’s bounding box in the image feature map f</p>
<script type="math/tex; mode=display">
\mathbf{g}(x, y, z)=\frac{1}{\left(u_{2}-u_{1}\right)\left(v_{2}-v_{1}\right)} \sum_{u=u_{1}}^{u_{2}} \sum_{v=v_{1}}^{v_{2}} \mathbf{f}(u, v)</script></li>
<li><p>But this is extremely <strong>memory intensive.</strong></p>
</li>
</ul>
</li>
<li><p>applications such as autonomous driving where most objects are <strong>ﬁxed to the 2D ground plane</strong>, we can make the problem more tractable by collapsing the 3D voxel feature map down to a <strong>third, two-dimensional representation</strong> which we term the <strong>orthographic feature map h(x, z).</strong></p>
<ul>
<li><p>The orthographic feature map is obtained by <strong>summing</strong> voxel features <strong>along the vertical axis</strong> after multiplication with a set of learned weight matrices.</p>
<script type="math/tex; mode=display">
\mathbf{h}(x, z)=\sum_{y=y_{0}}^{y_{0}+H} W(y) \mathbf{g}(x, y, z)</script></li>
<li><p>A major challenge with the above approach is the need to <strong>aggregate features over a very large number of regions.</strong></p>
</li>
</ul>
</li>
<li><p><strong>integral feature map</strong>, F, is constructed from an input feature map f using the recursive relation</p>
<script type="math/tex; mode=display">
F(u,v)=f(u,v)+F(u-1, v)+F(u, v-1)-F(u-1, v-1)</script><p>Then</p>
<script type="math/tex; mode=display">
g(x,y,z)=\frac{F(u_1,v_1)+F(u_2,v_2)-F(u_1,v_2)-F(u_2,v_1)}{(u_2-u_1)(v_2-v_1)}</script></li>
</ul>
</li>
<li><p>Topdown network: consisting of a series of ResNet <strong>residual units</strong>, processes the BEV feature maps <strong>in a manner</strong> which is <strong>invariant</strong> to the perspective effects observed in the <strong>image</strong>. Emphasize the importance of reasoning in 3D for <strong>object recognition and detection</strong> in complex 3D scenes.</p>
<ul>
<li>ResNet-style skip connections</li>
</ul>
</li>
<li><p>Output heads</p>
<ul>
<li><p><strong>Confidence map</strong>, S(x, z) is a <strong>smooth</strong> Gaussian function. l1 loss. </p>
<script type="math/tex; mode=display">
S(x, z)=\max _{i} \exp \left(-\frac{\left(x_{i}-x\right)^{2}+\left(z_{i}-z\right)^{2}}{2 \sigma^{2}}\right)</script><p><strong>vastly fewer positive</strong> (high conﬁdence) locations than negative ones, which leads to the negative component of the loss dominating optimization. It is coarse approximation. To overcome this we <strong>scale the loss</strong> corresponding to negative locations (which we deﬁne as those with S(x, z) &lt; 0.05) <strong>by a constant factor of 10 −2 .</strong></p>
</li>
<li><p>as a classification problem, with a cross entropy loss.</p>
</li>
<li><p><strong>Localization</strong> and bounding box estimation. In order to localize each object more precisely, l1 loss. </p>
<script type="math/tex; mode=display">
\boldsymbol{\Delta}_{\boldsymbol{p o s}}(x, z)=\left[\frac{x_{i}-x}{\sigma} \quad \frac{y_{i}-y_{0}}{\sigma} \quad \frac{z_{i}-z}{\sigma}\right]^{\top}</script></li>
<li><p>the <strong>dimension head</strong>, predicts the <strong>logarithmic scale offset</strong>. l1 loss. </p>
<script type="math/tex; mode=display">
\boldsymbol{\Delta}_{\text {dim }}(x, z)=\left[\log \frac{w_{i}}{\bar{w}} \quad \log \frac{h_{i}}{h} \quad \log \frac{l_{i}}{l}\right]^T</script></li>
<li><p>the <strong>orientation head</strong>, predicts the <strong>sine and cosine</strong>. l1 loss. </p>
<script type="math/tex; mode=display">
\boldsymbol{\Delta}_{\text {ang }}(x, z)=\left[\sin \theta_i  \quad  \cos\theta_i\right]^T</script></li>
</ul>
</li>
<li><p>Non-maximum suppression and decoding stage(peaks in the confidence maps and discrete bounding box predictions)</p>
<p>use of <strong>conﬁdence maps</strong> in place of anchor box classiﬁcation is that we can apply NMS in the <strong>more conventional</strong> image processing sense</p>
</li>
</ul>
<h2 id="M3D-RPN"><a href="#M3D-RPN" class="headerlink" title="M3D-RPN"></a>M3D-RPN</h2><p>Propose to reduce the gap by reformulating the monocular 3D detection problem as a <strong>standalone 3D region proposal network.</strong> Design <strong>depth-aware convolutional layers</strong> which enable location speciﬁc feature development and in consequence <strong>improved 3D scene understanding</strong>.</p>
<hr>
<p><img src="/2020/08/29/3DObjectDetection/Brazil_2019_1.png" alt=""></p>
<p><img src="/2020/08/29/3DObjectDetection/Brazil_2019_2.png" style="zoom:67%;"></p>
<p>Contributions:</p>
<ul>
<li><p>Formulate a standalone monocular 3D region proposal network (<strong>M3D-RPN</strong>) with a <strong><em>shared 2D and 3D detection</em></strong> space, while using <strong>prior statistics</strong> to serve as strong initialization for each 3D parameter. 将各个参数初始化为先验值，可以使得每一个独立的anchor有强大的3D先验知识。同时生成2D和3D的proposals</p>
<ul>
<li><p>Anchor</p>
<p>首先2D和3D是共享参数——中心点位置<script type="math/tex">[x,y]_P</script>的，来一起寻找proposal，即之后去求出<script type="math/tex">[w,h]_{2D},z_P, [w,h,l,\theta]_{3D}</script>。那么对于2Dbbox其实已经可以确定了，对于3D的框，用<script type="math/tex">[x,y,z]_P</script>可以得到空间中的中心点坐标，然后根据剩下4个信息就可以得到3D世界下的anchor了。这里的P是增加了一维[0,0,0,1].</p>
<script type="math/tex; mode=display">
\left[\begin{matrix}x\cdot z\\y\cdot z\\z\end{matrix}\right]_P=P\cdot\left[\begin{matrix}x\\y\\z\\1\end{matrix}\right]_{3D}</script><p>The outputs of <script type="math/tex">[t_x , t_y , t_w , t_h ]_{2D}</script>  represent the 2D bounding box transformation.</p>
<script type="math/tex; mode=display">
x'_{2D}=x_P+t_{x_{2D}}\cdot w_{2D}, \ y'_{2D}=y_P+t_{y_{2D}}\cdot h_{2D}, \\
w'_{2D}=exp(t_{w_{2D}})\cdot w_{2D}, \ 
h'_{2D}=exp(t_{h_{2D}})\cdot h_{2D}</script><p>3D outputs:</p>
<script type="math/tex; mode=display">
x'_{P}=x_P+t_{x_{P}}\cdot w_{2D}, \ y'_{P}=y_P+t_{y_{P}}\cdot h_{2D}, \  
z'_{P}=t_{z_{P}}\cdot z_{P}\\
w'_{3D}=exp(t_{w_{3D}})\cdot w_{3D}, \ 
h'_{3D}=exp(t_{h_{3D}})\cdot h_{3D}, \
l'_{3D}=exp(t_{l_{3D}})\cdot l_{3D}\\
\theta'_{3D}=t_{\theta_{3D}}\cdot \theta_{3D}</script></li>
<li><p>Loss</p>
<script type="math/tex; mode=display">
L=L_c+\lambda_1L_{b_{2D}}+\lambda_2L_{b_{3D}}\\
L_c=-\log(\frac{exp(c_\tau)}{\sum_i^{n_c}exp(c_i)})\\
L_{b_{2D}}=-\log(IoU(b'_{2D},\hat b_{2D}))\\
L_{b_{3D}}=SmoothL_1(b_{3D},\hat g_{3D})</script></li>
</ul>
</li>
<li><p>Propose <strong>depth-aware convolution</strong> to improve the 3D parameter estimation, thereby enabling the network to learn more <strong>spatially-aware high-level features.</strong> <strong>high-level features improve</strong> when <strong>given increased awareness of their depth</strong> and while <strong>assuming a consistent camera scene geometry.</strong></p>
<ul>
<li>Drawback: increase of <strong>memory</strong> footprint for a given layer by ×b.</li>
<li>connect two parallel paths at the end of the backbone network. <ul>
<li>uses <strong>regular convolution</strong> where kernels are <strong>shared spatially,</strong> which we refer to as <strong>global</strong>. </li>
<li>The second path exclusively uses <strong>depth-aware convolution</strong> and is referred to as <strong>local</strong>.</li>
</ul>
</li>
</ul>
</li>
<li><p>Propose a simple <strong>orientation estimation post-optimization algorithm</strong> which uses <strong>3D → 2D projection consistency loss</strong> within a post-optimization algorithm. </p>
<ul>
<li>using a learned attention <script type="math/tex">\alpha</script>.<script type="math/tex; mode=display">
O^i=O^i_{global}\cdot\alpha_i+O^i_{local}\cdot(1-\alpha_i)</script></li>
</ul>
<p><img src="/2020/08/29/3DObjectDetection/Brazil_2019_3.png" style="zoom:47%;"></p>
<p><img src="/2020/08/29/3DObjectDetection/Brazil_2019_4.png" style="zoom:47%;"></p>
</li>
</ul>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">generate_anchors()</div><div class="line">compute_bbox_stats()</div><div class="line">init_training_model()</div><div class="line"><span class="keyword">for</span> iteration <span class="keyword">in</span> range(start_iter, conf.max_iter):</div><div class="line">  iterator, images, imobjs = next_iteration()</div><div class="line">  adjust_lr()</div><div class="line">  cls, prob, bbox_2d, bbox_3d, feat_size = M3D_rpn_net()</div><div class="line">  det_loss, det_stats = criterion_det(cls, prob, bbox_2d, bbox_3d, imobjs, feat_size)</div><div class="line">  <span class="keyword">if</span> total_loss &gt; <span class="number">0</span>:</div><div class="line">			total_loss.backward()</div><div class="line">  compute_stats(tracker, stats)</div></pre></td></tr></table></figure>
<h2 id="MonoPSR"><a href="#MonoPSR" class="headerlink" title="MonoPSR"></a>MonoPSR</h2><h2 id="Pseudo-LiDAR"><a href="#Pseudo-LiDAR" class="headerlink" title="Pseudo-LiDAR"></a>Pseudo-LiDAR</h2><h2 id="3D-Deepbox"><a href="#3D-Deepbox" class="headerlink" title="3D-Deepbox"></a>3D-Deepbox</h2><h2 id="Joint-Monocular-3D-Vehicle-Detection-and-Tracking——2019"><a href="#Joint-Monocular-3D-Vehicle-Detection-and-Tracking——2019" class="headerlink" title="Joint Monocular 3D Vehicle Detection and Tracking——2019"></a>Joint Monocular 3D Vehicle Detection and Tracking——2019</h2><p>An online network architecture to <strong>jointly track and detect vehicles</strong> in 3D from a series of monocular images.The framework can not only associate detections of vehicles <strong>in motion over time</strong>, but also estimate their complete 3D bounding box information from <strong>a sequence of 2D images</strong> captured on a moving platform. Design <strong>a motion learning module</strong> based on an <strong>LSTM</strong> for more accurate long-term motion extrapolation. Robust <strong><em>tracking helps 3D detection</em></strong>. leverage novel <strong>occlusion-aware association and depth-ordering matching algorithms</strong> to overcome the occlusion and reappearance problems in tracking. <strong>Update their 3D poses</strong> using LSTM motion estimation along a trajectory, <strong>integrating</strong> single-frame observations associated with the instance over time.</p>
<hr>
<p><img src="/2020/08/29/3DObjectDetection/Tracking_2019_1.png" alt=""></p>
<p><strong>检测state：</strong></p>
<script type="math/tex; mode=display">
s_a=(P,O,D,F,\Delta P)</script><p>P代表position,<script type="math/tex">(x,y,z)</script>；<script type="math/tex">\Delta P</script>代表速度, <script type="math/tex">(\dot{x}, \dot{y}, \dot{z})</script>, O朝向角，D维度<script type="math/tex">(l,w,h)</script>，F代表appearance feature。M(X)表示投影的2D外围框。</p>
<p><strong>Assocation and Tracking</strong></p>
<p>通过前后帧同一object应该有部分重叠，并且相似性高这一特点。</p>
<script type="math/tex; mode=display">
\begin{array}{c}
\mathbf{A}_{\mathrm{deep}}\left(\tau_{a}, \mathbf{s}_{a}\right)=\exp \left(-\left\|F_{\tau_{a}}, F_{\mathbf{s}_{a}}\right\|_{1}\right) \\
\mathbf{A}_{2 \mathrm{D}}\left(\tau_{a}, \mathbf{s}_{a}\right)=\frac{\mathbf{d}_{\tau_{a}} \cap \mathbf{d}_{\mathbf{s}_{a}}}{\mathbf{d}_{\tau_{a}} \cup \mathbf{d}_{a}} \\
\mathbf{A}_{3 \mathrm{D}}\left(\tau_{a}, \mathbf{s}_{a}\right)=\mathbb{1} \times \frac{\phi\left(\mathbf{M}\left(X_{\tau_{a}}\right)\right) \cap \mathbf{M}\left(X_{\mathbf{s}_{a}}\right)}{\phi\left(\mathbf{M}\left(X_{\tau_{a}}\right)\right) \cup \mathbf{M}\left(X_{\mathbf{s}_{a}}\right)}\\
\left.\phi(\cdot)=\underset{x}{\arg \min }\left\{ x \mid \text { ord }(x)<\text { ord }\left(x_{0}\right) \forall x_{0} \in \mathbf{M}\left(X_{\tau_{a}}\right)\right)\right\}
\end{array}</script><p>其中A是affinity matrix，第一个参数表示存在的tracking序列，第二个参数表示候选的状态，然后最终的通过加权之和来评估相关性。1表示在depth过滤之后，是否被保留。<script type="math/tex">\phi</script>表示overlapping function。得到权重后，通过Kuhn-Munkres algorithm匈牙利KM算法来匹配。</p>
<p><strong>Motion model</strong></p>
<ul>
<li><p><strong>Prediction LSTM (P-LSTM)</strong> models</p>
<p>dynamic object location in 3D coordinates by <strong>predicting object velocity</strong> from previously updated velocities <script type="math/tex">\dot P_{T −n:T −1}</script> and the previous location <script type="math/tex">\bar P_{T − 1}</script>.</p>
</li>
<li><p><strong>Updating LSTM (U-LSTM)</strong> </p>
<p>considers both current <script type="math/tex">\hat P_T</script> and previously predicted location <script type="math/tex">\tilde P_{T −1}</script> to update the location and velocity.</p>
</li>
<li><p>handle missed and occluded objects</p>
<script type="math/tex; mode=display">
\mathbf{s}_{a}^{(i)}=\mathbf{s}_{a-1}^{(i)}+\alpha\left(\mathbf{s}_{a}^{*}-\mathbf{s}_{a-1}^{(i)}\right)\\
\alpha=1-\mathbf{A}_{\text {deep }}\left(\tau_{a}^{i}, \mathbf{s}_{a}^{*}\right)</script></li>
<li><p>Loss</p>
<ul>
<li><p>L1 loss</p>
<script type="math/tex; mode=display">
\mathrm{L}_{1}=\left|\dot{P}_{T}-\dot{P}_{T-1}\right|</script></li>
<li><p><strong>linear motion loss</strong>: smooth transition of location estimation.</p>
<script type="math/tex; mode=display">
 L_\text {linear}=\left(1-\cos \left(P_{T}, P_{T-1}\right)\right)+L 1\left(P_{T}, P_{T-1}\right)</script></li>
</ul>
</li>
</ul>
<p><strong>Estimation:</strong> </p>
<ul>
<li><p><strong>3D Estimation</strong></p>
<ul>
<li><p>A Dimension Score (DS)</p>
<script type="math/tex; mode=display">
\mathcal{D} S=\min \left(\frac{V_{\mathrm{pred}}}{V_{\mathrm{gt}}}, \frac{V_{\mathrm{gt}}}{V_{\mathrm{pred}}}\right)</script></li>
<li><p>A Center Score (CS)</p>
<script type="math/tex; mode=display">
C S=\left(1+\cos \left(a_{\mathrm{gt}}-a_{\mathrm{pd}}\right)\right) / 2</script></li>
</ul>
</li>
<li><p><strong>Tracking</strong></p>
<ul>
<li>multiple object tracking accuracy (MOTA)</li>
<li>multiple object tracking precision (MOTP)</li>
<li>miss-match (MM), false positive (FP)</li>
<li>false negative (FN)</li>
</ul>
</li>
</ul>
<h2 id="Accurate-Monocular-Object-Detection-via-Color-Embedded-3D-Reconstruction-for-Autonomous-Driving——2019"><a href="#Accurate-Monocular-Object-Detection-via-Color-Embedded-3D-Reconstruction-for-Autonomous-Driving——2019" class="headerlink" title="Accurate Monocular Object Detection via Color-Embedded 3D Reconstruction for Autonomous Driving——2019"></a>Accurate Monocular Object Detection via Color-Embedded 3D Reconstruction for Autonomous Driving——2019</h2><p>the performance gap not only due to the <strong><em>accuracy of the data</em></strong>, but also its <strong><em>representation</em></strong></p>
<hr>
<p><img src="/2020/08/29/3DObjectDetection/ColorEmbedded_1.png" alt=""></p>
<ul>
<li><p>3D data Generation</p>
<p>leverage a stand-alone module to transform the input data <strong><em>from 2D image plane</em></strong> to <strong><em>3D point clouds space</em></strong> for a better input representation with the help of <strong>camera calibration ﬁles</strong> in order to give the 3D information explicitly.</p>
<p>The <strong>beneﬁts</strong> for transform depth map into <strong>point cloud</strong> can be enumerated as follow: </p>
<ol>
<li>Point cloud data shows the <strong>spatial information explicitly</strong>, which make it <strong>easier</strong> for network to <strong>learn the non-linear mapping</strong> from input to output.</li>
<li><strong>Richer features</strong> can be learnt by the network because some <strong>speciﬁc spatial structures</strong> <strong>exist only in 3D space</strong>.</li>
<li>The recent signiﬁcant <strong>progress of deep learning</strong> on point clouds provides a solid building brick, which we can estimate 3D detection results in a more effective and efficient way.</li>
</ol>
<ul>
<li>trained <strong>two deep CNNs</strong> to do intermediate tasks:<ul>
<li>2D detection  to get <strong>position</strong> information, the <strong>conﬁdence scores</strong> of 2D boxes are assigned to their corresponding 3D boxes.</li>
<li>depth estimation to get <strong>depth</strong> information(more on <strong>how to</strong> use depth information than on how to get them)</li>
</ul>
</li>
<li>use <strong>2D bounding box</strong> to get the <strong>prior information</strong> about the <strong>location</strong> of the RoI</li>
<li>extract the <strong>points</strong> in each RoI as input data for subsequent steps</li>
</ul>
</li>
<li><p>3D Box Estimation</p>
<p>perform the 3D detection using <strong>PointNet backbone net</strong> to obtain objects’ 3D locations, dimensions and orientations.</p>
<p><strong>multi-modal features fusion module</strong> to embed the complementary RGB cue into the generated point clouds representation. </p>
<ul>
<li><p>Input</p>
<p>The <strong>input point cloud S</strong> can be generated using depth map and 2D bounding box B as follow:</p>
<script type="math/tex; mode=display">
S=\{p \mid p \leftarrow \mathbf{F}(v), v \in \mathbf{B}\}\\
F=\left\{\begin{array}{l}
z=d \\
x=\left(u-C_{x}\right) * z / f \\
y=\left(v-C_{y}\right) * z / f
\end{array}\right.</script><p>v is the pixel in depth map.</p>
</li>
<li><p>Segmentation: based on <strong>depth prior</strong> to segment the points.</p>
<ul>
<li><p>compute the <strong>depth mean</strong> in each 2D bounding box in order to get the approximate position of RoI</p>
</li>
<li><p>use it as the <strong>threshold</strong></p>
</li>
<li><p>All points with Z-channel value <strong>greater than this threshold</strong> are considered as <strong>background points</strong></p>
<script type="math/tex; mode=display">
S^{\prime}=\left\{p \mid p_{v} \leq \frac{\sum_{p \in S} p_{v}}{|S|}+r, p \in S\right\}</script><p><script type="math/tex">p_v</script> denotes the Z-channel value (which is equal to depth). r is a <strong>bias</strong> used to correct the threshold.</p>
<p><strong>randomly select</strong> a <strong>ﬁxed number of points</strong> in point set S0 as the output of this module in order to ensuring consistency of number of subsequent network’s input points.</p>
</li>
</ul>
</li>
<li><p>Det-Net</p>
<p><img src="/2020/08/29/3DObjectDetection/ColorEmbedded_2.png" alt=""></p>
<p>predict <strong>the center δ of RoI</strong> using a <strong>lightweight network</strong> and use it to <strong>update the point cloud</strong> as follow:</p>
<script type="math/tex; mode=display">
S^{\prime \prime}=\left\{p \mid p-\delta, p \in S^{\prime}\right\}</script><p>Estimate here is a <strong>’residual’ center</strong>, which means the real center is C + δ.</p>
<p><strong>RGB Information Aggregation</strong>: aggregate complementary RGB information to point cloud.</p>
<script type="math/tex; mode=display">
S=\{p \mid p \leftarrow \mathbf{F}(v), D(v), v \in \mathbf{B}\}</script><p>D is a function which output the corresponding RGB values of input point. <strong>6D vectors: [x, y, z, r, g, b].</strong></p>
<p>utilize the <strong>attention mechanism</strong> for guiding the message passing between the <strong>spatial features and RGB features.</strong> the attention can act <strong>as a gate function</strong>.</p>
<p><strong>An attention map G</strong> is ﬁrst produced from the feature maps F generated from XYZ branch as follow:</p>
<script type="math/tex; mode=display">
\mathbf{G} \leftarrow \sigma\left(f\left(\left[\mathbf{F}_{\max }^{x y z}, \mathbf{F}_{a v g}^{x y z}\right]\right)\right)</script><p>f is the nonlinear function learned from a convolution layer and σ is a sigmoid function for normalizing the attention map.</p>
<script type="math/tex; mode=display">
\mathbf{F}^{x y z} \leftarrow \mathbf{F}^{x y z}+\mathbf{G} \odot \mathbf{F}^{r g b}</script></li>
</ul>
</li>
</ul>
<p>simultaneously optimize the two networks for 3D detection jointly with a <strong>multi-task loss function</strong>:</p>
<script type="math/tex; mode=display">
L=L_{loc}+L_{det}+\lambda L_{corner}</script><hr>
<p><strong>Backwards:</strong></p>
<p>Because it’s a <strong>2D-driven</strong> framework, the proposed method will <strong>fail if the 2D box is a false positive sample or missing.</strong></p>
<h2 id="MonoDIS-Disentangling-Monocular-3D-Object-Detection——2019"><a href="#MonoDIS-Disentangling-Monocular-3D-Object-Detection——2019" class="headerlink" title="MonoDIS: Disentangling Monocular 3D Object Detection——2019"></a>MonoDIS: Disentangling Monocular 3D Object Detection——2019</h2><p><strong>Disentangling transformation</strong> for 2D and 3D detection losses and <strong>a novel, self-supervised conﬁdence score</strong> for 3D bounding boxes. Our proposed loss disentanglement has the twofold advantage of <strong>simplifying the training dynamics</strong> in the presence of losses with complex interactions of parameters, and sidestepping the issue of <strong>balancing independent regression terms</strong>. Our solution overcomes these issues by <strong>isolating</strong> the contribution made by <strong>groups of parameters</strong> to a given loss, without changing its nature.</p>
<p>本文的核心是提出了<strong>解耦的 regression loss</strong>，用来替代之前同时回归 center、size、rotation 带来的由于<strong>各个 opponent 的 loss 大小不同</strong>导致的训练问题；基本思想是将回归的部分分成 k 个 group，每个 <strong>group 只有自身</strong>的参数需要学习，其他的部分<strong>使用 gt 代替</strong>，从而实现<strong>每个分支只回归某一个 component</strong>，使得训练更加<strong>稳定</strong>。同时<strong>提出了改进的 sIoU loss</strong>，将<strong>没有 overlap 的 bboxes 的 loss</strong> 也考虑进来。同时，本文使用 <strong>memory efficient in-place synced bn</strong> 替换了原来的 BatchNorm，从而更 efficient 的训练；</p>
<p><del>这么一看，这篇文章解决了我的很多问题，第一个问题回归三个信息的大小不同问题，在我的实验时也发现了此问题，但是我的解决方案就是简单地用函数包装一些信息，使得他们的范围相差不大；但是这样做实际上是将大范围的预测变得粗略化了；第二个问题，改进的sIoU loss，这一个问题是被学长提问的时候发现的，我对这一段的处理是采用了2D检测的方法，有了Prec来表示，在训练时用heatmap的loss来计算，但是这样做，在评估的时候相当于多个独立的评估，不在一个整体之中，而且对于loss之间的一个weight问题相当于不用考虑？；第三个问题阅读之前还是不清楚是什么问题。这么一看2019年的文章，这些问题我可以发现，但是我没有去思考一个更好的方法，而是去找个短暂的解决方案。</del></p>
<hr>
<p><strong>Backbone:</strong></p>
<p><img src="/2020/08/29/3DObjectDetection/MonoDIS_2019_1.png" alt=""></p>
<p><strong>2D Detection Head:</strong></p>
<p><img src="/2020/08/29/3DObjectDetection/MonoDIS_2019_2.png" alt=""></p>
<p>其中，在2D bounding box的置信度为<script type="math/tex">p\in[0,1]</script></p>
<script type="math/tex; mode=display">
p_{2 \mathrm{D}}=\left(1+e^{-\zeta_{2 \mathrm{D}}}\right)^{-1}</script><p>分析可得，<script type="math/tex">\zeta_{2 \mathrm{D}}</script>趋向于正无穷时，置信度趋向于1；但<script type="math/tex">\zeta_{2 \mathrm{D}}</script>趋向于负无穷时，置信度趋向于0。</p>
<p>根据图像坐标系分成的cell g的中心点为<script type="math/tex">(u_g, v_g)</script>， 那么bounding box的中心点为</p>
<script type="math/tex; mode=display">
\left(u_{b}, v_{b}\right)=\left(u_{g}+\delta_{u} w_{a}, v_{g}+\delta_{v} h_{a}\right)</script><p>bounding box的大小为</p>
<script type="math/tex; mode=display">
\left(w_{b}, h_{b}\right)=\left(w_{a} e^{\delta w}, h_{a} e^{\delta_{h}}\right)</script><p>Use <strong>focal loss</strong> to train the bounding box <strong>confidence score.</strong></p>
<script type="math/tex; mode=display">
L_{2 \mathrm{D}}^{\mathrm{conf}}\left(p_{2 \mathrm{D}}, y\right)=-\alpha y\left(1-p_{2 \mathrm{D}}\right)^{\gamma} \log p_{2 \mathrm{D}}-\bar{\alpha} \bar{y} p_{2 \mathrm{D}}^{\gamma} \log \left(1-p_{2 \mathrm{D}}\right)\\
\bar \alpha=1-\alpha,\quad \bar y=1-y</script><p>其中，y是target confidence <script type="math/tex">y\in \{0, 1\}</script>. <script type="math/tex">\alpha\in[0,1], \gamma>0</script>超参数。</p>
<p><strong>Detection loss:</strong></p>
<script type="math/tex; mode=display">
L_{2 \mathrm{D}}^{\mathrm{bb}}(\boldsymbol{b}, \hat{\boldsymbol{b}})=1-\operatorname{sIoU}(\boldsymbol{b}, \hat{\boldsymbol{b}})</script><script type="math/tex; mode=display">
\operatorname{sIoU}(\boldsymbol{b}, \hat{\boldsymbol{b}})=\frac{|\boldsymbol{b} \sqcap \hat{\boldsymbol{b}}|_{\pm}}{|\boldsymbol{b}|+|\hat{\boldsymbol{b}}|-|\boldsymbol{b} \sqcap \hat{\boldsymbol{b}}|_{\pm}}</script><p>where</p>
<script type="math/tex; mode=display">
\boldsymbol{b} \sqcap \hat{\boldsymbol{b}}=\left(\begin{array}{c}
\max \left(u_{1}, \hat{u}_{1}\right) \\
\max \left(v_{1}, \hat{v}_{1}\right) \\
\min \left(u_{2}, \hat{u}_{2}\right) \\
\min \left(v_{2}, \hat{v}_{2}\right)
\end{array}\right)</script><p>b是bounding box. <strong>sIoU</strong> represents an extension of the common IoU function, which prevents gradients from vanishing in case of non-overlapping bounding boxes. call it <strong>signed IoU</strong>.</p>
<p><img src="/2020/08/29/3DObjectDetection/MonoDIS_2019_4.jpg" alt=""></p>
<p>其中正负号定义：</p>
<script type="math/tex; mode=display">
|\boldsymbol{b}|_{\pm}=\left\{\begin{array}{ll}
+|\boldsymbol{b}| & \text { if } u_{2}>u_{1} \text { and } v_{2}>v_{1} \\
-|\boldsymbol{b}| & \text { otherwise }
\end{array}\right.</script><p><strong>3D Detection Head:</strong></p>
<p><img src="/2020/08/29/3DObjectDetection/MonoDIS_2019_3.png" alt=""></p>
<script type="math/tex; mode=display">
\theta=\left(\delta z, \Delta_{u}, \Delta_{v}, \delta_{W}, \delta_{H}, \delta_{D}, q_{r}, q_{i}, q_{j}, q_{k}\right)</script><p>其中，对于给出的2D proposal预测的3D bounding box的置信度为【公司不能用，应该是因为3D信息的标注并不是对于全部object；并且其真实置信度是根据能学习到的bbox来确定的】</p>
<script type="math/tex; mode=display">
p_{3 \mathrm{D} \mid 2 \mathrm{D}}=\left(1+e^{-\zeta_{3 \mathrm{D}}}\right)^{-1}</script><p>中心点的深度</p>
<script type="math/tex; mode=display">
z=\mu_z+\sigma_z\delta_z</script><p>中心点在图像上的投影位置</p>
<script type="math/tex; mode=display">
c=(u_b+\Delta_u,v_b+\Delta_v)</script><p>3D大小同样也是有先验值的基础上的</p>
<script type="math/tex; mode=display">
\boldsymbol{s}=\left(W_{0} e^{\delta_{W}}, H_{0} e^{\delta_{H}}, D_{0} e^{\delta_{D}}\right)</script><p>pose</p>
<script type="math/tex; mode=display">
\boldsymbol{q}=q_{r}+q_{i} \mathrm{i}+q_{j} \mathrm{j}+q_{k} \mathrm{k}</script><p><strong>Detection loss:</strong></p>
<script type="math/tex; mode=display">
L_{3 \mathrm{D}}^{\mathrm{bb}}(\boldsymbol{B}, \hat{\boldsymbol{B}})=\frac 1 8\|B-\hat B\|_H</script><p>其中，<script type="math/tex">\|\cdot\|_H</script> denotes the Huber loss.</p>
<p>对于3D置信度预测的损失，则是通过自主学习，计算它的loss仍然通过the standard <strong>binary cross entropy loss</strong>；它的真实值通过一个转换得到</p>
<script type="math/tex; mode=display">
\hat{p}_{3 \mathrm{D} \mid 2 \mathrm{D}}=e^{-\frac{1}{T} L_{3 \mathrm{D}}^{\mathrm{bb}}(B, \hat{B})}\\
L_{3 \mathrm{D}}^{\mathrm{conf}}\left(p_{3 \mathrm{D} \mid 2 \mathrm{D}}, \hat{p}_{3 \mathrm{D} \mid 2 \mathrm{D}}\right)=-\hat{p} \log p-(1-\hat{p}) \log (1-p)</script><p>根据贝叶斯，可以得到3D置信度：</p>
<script type="math/tex; mode=display">
p_{3D}=p_{3D|2D}p_{2D}</script><p>这样在预测阶段的时候，通过阈值<script type="math/tex">\tau_{conf}</script>过滤掉框，后面就不用NMS或者3D先验知识后处理了。</p>
<hr>
<p>Car size of <script type="math/tex">W_0 = 1.53m, H_0 = 1.63m, D_0 = 3.88m</script> and depth statistics of <script type="math/tex">\mu_z = 28.01m</script> and <script type="math/tex">\sigma_ z = 16.32m</script>. We ﬁltered the ﬁnal 3D detections with a score threshold of <script type="math/tex">\tau_{conf} = 0.05</script>.</p>
<h2 id="GS3D-An-Efﬁcient-3D-Object-Detection-Framework-for-Autonomous-Driving——2019"><a href="#GS3D-An-Efﬁcient-3D-Object-Detection-Framework-for-Autonomous-Driving——2019" class="headerlink" title="GS3D: An Efﬁcient 3D Object Detection Framework for Autonomous Driving——2019"></a>GS3D: An Efﬁcient 3D Object Detection Framework for Autonomous Driving——2019</h2><p>Leveraging the off-the-shelf <strong>2D object detector</strong>, we propose an artful approach to efﬁciently obtain a <strong>coarse cuboid</strong> for each predicted 2D box. The coarse cuboid has enough accuracy to <strong>guide</strong> us to determine the 3D box of the object by <strong>reﬁnement</strong>. we explore the 3D structure information of the object by employing the <strong>visual features of visible surfaces</strong>.</p>
<hr>
<p><img src="/2020/08/29/3DObjectDetection/GS3D_2019_1.png" alt=""></p>
<ul>
<li><p>首先用成熟的2D检测器，得到2Dbbox、类别和方向；</p>
<p><img src="/2020/08/29/3DObjectDetection/GS3D_2019_3.png" style="zoom:50%;"></p>
</li>
<li><p>然后确定它的basic cuboid；</p>
</li>
<li><p>最后从可见的面上提取特征，进行refinement</p>
<p>从可见面上提取而不是2D框上提取是因为，相同2D框不同朝向的cubic应该有不同的置信度。</p>
<p>可见面的表面特征用了数据增强；因为同一个bbox的朝向是很重要的</p>
</li>
</ul>
<p><strong>Discrete classiﬁcation based methods</strong> with <strong><em>quality aware loss</em></strong>(the more accurate target box gets the higher score) perform much <strong>better</strong> than <strong>direct regression</strong> approaches for the task of 3D box reﬁnement.</p>
<script type="math/tex; mode=display">
q=\left\{\begin{array}{ll}
1 & \text { if } o v>0.75 \\
0 & \text { if } o v<0.25 \\
2 o v-0.5 & \text { otherwise }
\end{array}\right.</script><p>ov is the 3D overlap between the target box and ground-truth.</p>
<ul>
<li><strong>dimension</strong>可以根据2Dbbox生成的<strong><em>类别</em></strong>信息，得到一个<strong>先验</strong>的值。</li>
<li>根据2D bbox顶部和底部的<strong>中心点</strong>，可以得到3D坐标下归一化的坐标值，从而<strong>高</strong>。底部中心点上增加了一个微小的抖动，由统计得到。</li>
<li><strong>朝向角</strong>根据观察角加上位置偏向角的和得到。</li>
<li>根据角度，对应的<strong>可见面</strong>就可以得到。大于0说明正面可见，小于零的说明背面可见，在<script type="math/tex">(-\frac \pi 2,\frac \pi 2)</script>左面可见，剩下的就是右面可见。</li>
<li>回归残差问题转变为分类问题，bin-loss</li>
</ul>
<p><img src="/2020/08/29/3DObjectDetection/GS3D_2019_2.png" alt=""></p>
<p>use <strong>BCE</strong> as the loss function</p>
<script type="math/tex; mode=display">
L_{q u a l i t y}=-[q \log (p)+(1-q) \log (1-p)]</script><h1 id="something"><a href="#something" class="headerlink" title="something"></a>something</h1><p>要去了解的方法：</p>
<ul>
<li>Part-sensitive warping预测框不匹配的方法</li>
<li>Frustum-PointNet（F-PointNet视锥）</li>
<li>AVOD</li>
</ul>
<p>转载请注明出处，谢谢。<br><blockquote class="blockquote-center"><p>愿 我是你的小太阳</p>
</blockquote></p>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=30987373&auto=0&height=66"></iframe>

<!-- UY BEGIN -->
<p><div id="uyan_frame"></div></p>
<script type="text/javascript" src="http://v2.uyan.cc/code/uyan.js?uid=2142537"></script>

<!-- UY END -->

      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>买糖果去喽</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>Donate</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechat.jpg" alt="Mrs_empress WeChat Pay"/>
        <p>WeChat Pay</p>
      </div>
    

    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/paper/" rel="tag"><i class="fa fa-tag"></i> paper</a>
          
            <a href="/tags/Object-detection/" rel="tag"><i class="fa fa-tag"></i> Object detection</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/07/08/CVPR-2020/" rel="next" title="CVPR-2020">
                <i class="fa fa-chevron-left"></i> CVPR-2020
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/09/10/随想小记-2020-09-10/" rel="prev" title="随想小记-2020-09-10">
                随想小记-2020-09-10 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          
            <img class="site-author-image" itemprop="image"
              src="/images/avatar.jpg"
              alt="Mrs_empress" />
          
            <p class="site-author-name" itemprop="name">Mrs_empress</p>
            <p class="site-description motion-element" itemprop="description">Hope be better and better, wish be happy and happy!</p>
        </div>

        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
            
              <a href="/archives">
            
                <span class="site-state-item-count">126</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">51</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">89</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/mrsempress" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                    
                      GitHub
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.facebook.com/chenxi.huang.56211" target="_blank" title="FB Page">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                    
                      FB Page
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/u/3309079767?refer_flag=1001030001_&nick=Mrs_empress_阡沫昕&is_hot=1" target="_blank" title="微博">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                    
                      微博
                    
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              Links
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://tobiaslee.top" title="TobiasLee" target="_blank">TobiasLee</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://abcml.xin/" title="ZeZe" target="_blank">ZeZe</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://notes-hongbo.top" title="Bob" target="_blank">Bob</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="https://undefinedf.github.io/" title="Fjh" target="_blank">Fjh</a>
                </li>
              
            </ul>
          </div>
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#paper"><span class="nav-number">1.</span> <span class="nav-text">paper</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#MonoGRNet-A-Geometric-Reasoning-Network-for-Monocular-3D-Object-Localization——2019"><span class="nav-number">1.1.</span> <span class="nav-text">*MonoGRNet: A Geometric Reasoning Network for Monocular 3D Object Localization——2019</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Deep-3D-box-3d-Bounding-Box-Estimation-Using-Deep-Learning-and-Geometry——2017"><span class="nav-number">1.2.</span> <span class="nav-text">*Deep 3D box: 3d Bounding Box Estimation Using Deep Learning and Geometry——2017</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Mono3d：Monocular-3D-Object-Detection-for-Autonomous-Driving——2016"><span class="nav-number">1.3.</span> <span class="nav-text">Mono3d：Monocular 3D Object Detection for Autonomous Driving——2016</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RTM3D-Real-time-Monocular-3D-Detection-from-Object-Keypoints-for-Autonomous-Driving——2020"><span class="nav-number">1.4.</span> <span class="nav-text">RTM3D: Real-time Monocular 3D Detection from Object Keypoints for Autonomous Driving——2020</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#IDA-3D-Instance-Depth-Aware-3D-Object-Detection-from-Stereo-Vision-for-Autonomous-Driving"><span class="nav-number">1.5.</span> <span class="nav-text">IDA-3D: Instance-Depth-Aware 3D Object Detection from Stereo Vision for Autonomous Driving</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SMOKE-Single-Stage-Monocular-3D-Object-Detection-via-Keypoint-Estimation——2020"><span class="nav-number">1.6.</span> <span class="nav-text">SMOKE: Single-Stage Monocular 3D Object Detection via Keypoint Estimation——2020</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Shift-R-CNN"><span class="nav-number">1.7.</span> <span class="nav-text">Shift R-CNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GS3D"><span class="nav-number">1.8.</span> <span class="nav-text">GS3D</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#OFT-Orthographic-Feature-Transform-for-Monocular-3D-Object-Detection——2018"><span class="nav-number">1.9.</span> <span class="nav-text">OFT: Orthographic Feature Transform for Monocular 3D Object Detection——2018</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#M3D-RPN"><span class="nav-number">1.10.</span> <span class="nav-text">M3D-RPN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MonoPSR"><span class="nav-number">1.11.</span> <span class="nav-text">MonoPSR</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pseudo-LiDAR"><span class="nav-number">1.12.</span> <span class="nav-text">Pseudo-LiDAR</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3D-Deepbox"><span class="nav-number">1.13.</span> <span class="nav-text">3D-Deepbox</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Joint-Monocular-3D-Vehicle-Detection-and-Tracking——2019"><span class="nav-number">1.14.</span> <span class="nav-text">Joint Monocular 3D Vehicle Detection and Tracking——2019</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Accurate-Monocular-Object-Detection-via-Color-Embedded-3D-Reconstruction-for-Autonomous-Driving——2019"><span class="nav-number">1.15.</span> <span class="nav-text">Accurate Monocular Object Detection via Color-Embedded 3D Reconstruction for Autonomous Driving——2019</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MonoDIS-Disentangling-Monocular-3D-Object-Detection——2019"><span class="nav-number">1.16.</span> <span class="nav-text">MonoDIS: Disentangling Monocular 3D Object Detection——2019</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GS3D-An-Efﬁcient-3D-Object-Detection-Framework-for-Autonomous-Driving——2019"><span class="nav-number">1.17.</span> <span class="nav-text">GS3D: An Efﬁcient 3D Object Detection Framework for Autonomous Driving——2019</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#something"><span class="nav-number">2.</span> <span class="nav-text">something</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2017 &mdash; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Mrs_empress</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div>

  <span class="post-meta-divider">|</span>

  <div class="theme-info">Theme &mdash; <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.2</div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.2"></script>



  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>


  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	





  





  








  





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("73XX9zwrQOBeD6S0LGJO26Ac-gzGzoHsz", "92PFBxqwUfTSuVqrflFGaf5G");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
